{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seed= 11\n",
    "np.random.seed( seed )\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# utility\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep Learning\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo Sequence Prediction Problem\n",
    "\n",
    "In this problem, we have a random sequence of numbers, and we would like to always pick up a number at a certain index. For ex. we can always choose the third number in a sequence. It does not matter what the sequence is, a classification model must always return the third integer. \n",
    "\n",
    "The purpose of investigating this problem with Vanila LSTM models is to demonstrate their capability of memorizing a certain element in a sequence.\n",
    "\n",
    "As the first step, let's first create a random sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# generate a sequence of random numbers in [0, n_features)\n",
    "def generate_sequence(length, n_features):\n",
    "    \"\"\"\n",
    "    length tells us the length of the sequence\n",
    "    n_features is the upper limit for random numbers\n",
    "    \n",
    "    ex. \n",
    "    length= 5, n_features= 10\n",
    "    sequence -> [2, 5, 1, 9, 6]\n",
    "    \"\"\"\n",
    "    return [ np.random.randint(0, n_features) for _ in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode( sequence, n_features):\n",
    "    \"\"\"\n",
    "    We can also do the same with keras.utils.to_categorical\n",
    "    However, keras method will set the number of features to the biggest number in the sequence, which\n",
    "    can be wrong because we are creating random numbers and sometimes the biggest number could not \n",
    "    show up.\n",
    "    However, if we have all the sequences, then the method would still works.\n",
    "    \"\"\"\n",
    "    encoding= list()\n",
    "    for value in sequence:\n",
    "        vector= [0 for _ in range(n_features)]\n",
    "        vector[value]= 1\n",
    "        encoding.append( vector )\n",
    "    return np.array(encoding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1:  [9, 0, 1, 7, 1]\n",
      "Sequence 1:  [7, 2, 8, 0, 0]\n",
      "\n",
      "One hot encoding of seq1: \n",
      "[[0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "One hot encoding of seq2: \n",
      "[[0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "seq1= generate_sequence(length= 5, n_features= 10)\n",
    "seq2= generate_sequence(length= 5, n_features= 10)\n",
    "\n",
    "print('Sequence 1: ', seq1)\n",
    "print('Sequence 1: ', seq2)\n",
    "\n",
    "print('\\nOne hot encoding of seq1: ')\n",
    "print(one_hot_encode(seq1, n_features= 10))\n",
    "print('\\nOne hot encoding of seq2: ')\n",
    "print(one_hot_encode(seq2, n_features= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see to_categorical does the same\n",
    "to_categorical( [seq1, seq2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    \n",
    "    \"\"\"\n",
    "    return [ np.argmax(vector) for vector in encoded_seq ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding of sequence 1: \n",
      " [[0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 0, 1, 7, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_seq= one_hot_encode(seq1, n_features= 10)\n",
    "print('One hot encoding of sequence 1: \\n', encoded_seq)\n",
    "one_hot_decode( encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_example(length, n_features, out_index):\n",
    "    \"\"\"\n",
    "    This fucntion generates one input for our Echo Sequence Prediction problem\n",
    "    \"\"\"\n",
    "    # generate sequence\n",
    "    sequence= generate_sequence(length, n_features)\n",
    "    encoded= one_hot_encode(sequence, n_features)\n",
    "    X= encoded.reshape( (1, length, n_features))\n",
    "    y= encoded[out_index, :].reshape( 1, n_features )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (1, 60, 100)\n",
      "y shape:  (1, 100)\n"
     ]
    }
   ],
   "source": [
    "length= 60\n",
    "n_features= 100\n",
    "out_index= 43\n",
    "X, y= generate_example(length, n_features, out_index)\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "=================================================================\n",
      "Total params: 260,900\n",
      "Trainable params: 260,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add( LSTM(200, input_shape= (X.shape[1], X.shape[2])))\n",
    "model.add( Dense(n_features, activation= 'softmax'))\n",
    "model.compile( loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 539.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (5000, 60, 100)\n",
      "y shape (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "X, y= list(), list()\n",
    "for i in tqdm(range(5000)):\n",
    "    X_temp, y_temp= generate_example(length, n_features, out_index)\n",
    "    X.append(X_temp)\n",
    "    y.append(y_temp)\n",
    "\n",
    "X= np.concatenate(X)\n",
    "y= np.concatenate(y)\n",
    "\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 4.6040 - acc: 0.0106\n",
      "Epoch 2/50\n",
      " - 2s - loss: 4.5811 - acc: 0.0234\n",
      "Epoch 3/50\n",
      " - 2s - loss: 4.5646 - acc: 0.0342\n",
      "Epoch 4/50\n",
      " - 2s - loss: 4.5267 - acc: 0.0374\n",
      "Epoch 5/50\n",
      " - 2s - loss: 4.4848 - acc: 0.0448\n",
      "Epoch 6/50\n",
      " - 2s - loss: 4.4213 - acc: 0.0502\n",
      "Epoch 7/50\n",
      " - 2s - loss: 4.3401 - acc: 0.0570\n",
      "Epoch 8/50\n",
      " - 2s - loss: 4.3225 - acc: 0.0596\n",
      "Epoch 9/50\n",
      " - 2s - loss: 4.2450 - acc: 0.0634\n",
      "Epoch 10/50\n",
      " - 2s - loss: 3.9831 - acc: 0.0958\n",
      "Epoch 11/50\n",
      " - 2s - loss: 3.7369 - acc: 0.1290\n",
      "Epoch 12/50\n",
      " - 2s - loss: 3.5550 - acc: 0.1558\n",
      "Epoch 13/50\n",
      " - 3s - loss: 3.2970 - acc: 0.1994\n",
      "Epoch 14/50\n",
      " - 2s - loss: 3.0604 - acc: 0.2404\n",
      "Epoch 15/50\n",
      " - 3s - loss: 2.7642 - acc: 0.2996\n",
      "Epoch 16/50\n",
      " - 2s - loss: 2.6260 - acc: 0.3220\n",
      "Epoch 17/50\n",
      " - 2s - loss: 2.3608 - acc: 0.3808\n",
      "Epoch 18/50\n",
      " - 2s - loss: 2.1920 - acc: 0.4110\n",
      "Epoch 19/50\n",
      " - 2s - loss: 1.9782 - acc: 0.4650\n",
      "Epoch 20/50\n",
      " - 2s - loss: 1.6221 - acc: 0.5516\n",
      "Epoch 21/50\n",
      " - 2s - loss: 1.5671 - acc: 0.5598\n",
      "Epoch 22/50\n",
      " - 2s - loss: 1.2062 - acc: 0.6622\n",
      "Epoch 23/50\n",
      " - 2s - loss: 1.0840 - acc: 0.6920\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.9385 - acc: 0.7372\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.6465 - acc: 0.8320\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.7991 - acc: 0.7710\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.4023 - acc: 0.9096\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.2659 - acc: 0.9556\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.1887 - acc: 0.9754\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.1553 - acc: 0.9838\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.1023 - acc: 0.9936\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.0871 - acc: 0.9936\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.0504 - acc: 0.9996\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.0403 - acc: 0.9998\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0189 - acc: 1.0000\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.0052 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdbd87a1f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs= 50, batch_size= 128, verbose= 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 553.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3713809025287628, 0.8799999952316284]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test= list(), list()\n",
    "for i in tqdm(range(100)):\n",
    "    X_temp, y_temp= generate_example(length, n_features, out_index)\n",
    "    X_test.append(X_temp)\n",
    "    y_test.append(y_temp)\n",
    "\n",
    "X_test= np.concatenate(X_test)\n",
    "y_test= np.concatenate(y_test)\n",
    "\n",
    "yhat = model.predict(X_test)\n",
    "#print('Sequence: %s' % [one_hot_decode(x) for x in X_test])\n",
    "#print('Expected: %s' % one_hot_decode(y_test))\n",
    "#print('Predicted: %s' % one_hot_decode(yhat))\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 5000 examples, we achieved a a test accuracy of 87 %\n",
    "\n",
    "### To improve test accuracy, we can either add more LSTM units or increase the nuber of samples. Let's try both. \n",
    "\n",
    "### First we increase the number of LSTM units from 200 to 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               40100     \n",
      "=================================================================\n",
      "Total params: 841,700\n",
      "Trainable params: 841,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add( LSTM(400, input_shape= (X.shape[1], X.shape[2])))\n",
    "model.add( Dense(n_features, activation= 'softmax'))\n",
    "model.compile( loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 4.6031 - acc: 0.0116\n",
      "Epoch 2/50\n",
      " - 2s - loss: 4.5750 - acc: 0.0288\n",
      "Epoch 3/50\n",
      " - 2s - loss: 4.5424 - acc: 0.0322\n",
      "Epoch 4/50\n",
      " - 2s - loss: 4.5020 - acc: 0.0468\n",
      "Epoch 5/50\n",
      " - 2s - loss: 4.4824 - acc: 0.0430\n",
      "Epoch 6/50\n",
      " - 2s - loss: 4.4470 - acc: 0.0480\n",
      "Epoch 7/50\n",
      " - 2s - loss: 4.4023 - acc: 0.0548\n",
      "Epoch 8/50\n",
      " - 2s - loss: 4.3469 - acc: 0.0604\n",
      "Epoch 9/50\n",
      " - 2s - loss: 4.2367 - acc: 0.0700\n",
      "Epoch 10/50\n",
      " - 2s - loss: 4.0491 - acc: 0.0964\n",
      "Epoch 11/50\n",
      " - 2s - loss: 3.7631 - acc: 0.1334\n",
      "Epoch 12/50\n",
      " - 2s - loss: 3.4961 - acc: 0.1796\n",
      "Epoch 13/50\n",
      " - 2s - loss: 3.0884 - acc: 0.2346\n",
      "Epoch 14/50\n",
      " - 2s - loss: 2.7901 - acc: 0.2910\n",
      "Epoch 15/50\n",
      " - 3s - loss: 2.4497 - acc: 0.3582\n",
      "Epoch 16/50\n",
      " - 2s - loss: 2.3362 - acc: 0.3846\n",
      "Epoch 17/50\n",
      " - 3s - loss: 1.7745 - acc: 0.5108\n",
      "Epoch 18/50\n",
      " - 2s - loss: 1.3298 - acc: 0.6126\n",
      "Epoch 19/50\n",
      " - 2s - loss: 1.0508 - acc: 0.6910\n",
      "Epoch 20/50\n",
      " - 2s - loss: 1.4129 - acc: 0.6184\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.5162 - acc: 0.8582\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.5274 - acc: 0.8490\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.1716 - acc: 0.9692\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.0951 - acc: 0.9832\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.3943 - acc: 0.9026\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.5074 - acc: 0.8542\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.1553 - acc: 0.9626\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.0382 - acc: 0.9988\n",
      "Epoch 29/50\n",
      " - 2s - loss: 0.0167 - acc: 1.0000\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 31/50\n",
      " - 2s - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 44/50\n",
      " - 2s - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.0018 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdcaf619358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs= 50, batch_size= 128, verbose= 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2929810881614685, 0.699999988079071]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 100 LSTM units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 4.6056 - acc: 0.0094\n",
      "Epoch 2/50\n",
      " - 2s - loss: 4.5922 - acc: 0.0174\n",
      "Epoch 3/50\n",
      " - 2s - loss: 4.5778 - acc: 0.0208\n",
      "Epoch 4/50\n",
      " - 2s - loss: 4.5584 - acc: 0.0250\n",
      "Epoch 5/50\n",
      " - 2s - loss: 4.5322 - acc: 0.0366\n",
      "Epoch 6/50\n",
      " - 2s - loss: 4.4965 - acc: 0.0298\n",
      "Epoch 7/50\n",
      " - 2s - loss: 4.4456 - acc: 0.0428\n",
      "Epoch 8/50\n",
      " - 2s - loss: 4.3804 - acc: 0.0470\n",
      "Epoch 9/50\n",
      " - 2s - loss: 4.2788 - acc: 0.0532\n",
      "Epoch 10/50\n",
      " - 2s - loss: 4.1701 - acc: 0.0666\n",
      "Epoch 11/50\n",
      " - 2s - loss: 4.0674 - acc: 0.0824\n",
      "Epoch 12/50\n",
      " - 2s - loss: 3.9470 - acc: 0.0984\n",
      "Epoch 13/50\n",
      " - 2s - loss: 3.8448 - acc: 0.1202\n",
      "Epoch 14/50\n",
      " - 2s - loss: 3.6758 - acc: 0.1338\n",
      "Epoch 15/50\n",
      " - 2s - loss: 3.5438 - acc: 0.1634\n",
      "Epoch 16/50\n",
      " - 3s - loss: 3.4209 - acc: 0.1836\n",
      "Epoch 17/50\n",
      " - 2s - loss: 3.3479 - acc: 0.1974\n",
      "Epoch 18/50\n",
      " - 3s - loss: 3.1729 - acc: 0.2270\n",
      "Epoch 19/50\n",
      " - 2s - loss: 3.0172 - acc: 0.2560\n",
      "Epoch 20/50\n",
      " - 2s - loss: 2.9538 - acc: 0.2644\n",
      "Epoch 21/50\n",
      " - 2s - loss: 2.7936 - acc: 0.3044\n",
      "Epoch 22/50\n",
      " - 2s - loss: 2.6819 - acc: 0.3196\n",
      "Epoch 23/50\n",
      " - 2s - loss: 2.5014 - acc: 0.3658\n",
      "Epoch 24/50\n",
      " - 2s - loss: 2.4520 - acc: 0.3648\n",
      "Epoch 25/50\n",
      " - 2s - loss: 2.2970 - acc: 0.4092\n",
      "Epoch 26/50\n",
      " - 2s - loss: 2.1090 - acc: 0.4562\n",
      "Epoch 27/50\n",
      " - 2s - loss: 2.0142 - acc: 0.4680\n",
      "Epoch 28/50\n",
      " - 2s - loss: 1.9077 - acc: 0.4908\n",
      "Epoch 29/50\n",
      " - 2s - loss: 1.7368 - acc: 0.5452\n",
      "Epoch 30/50\n",
      " - 2s - loss: 1.6005 - acc: 0.5792\n",
      "Epoch 31/50\n",
      " - 2s - loss: 1.4451 - acc: 0.6216\n",
      "Epoch 32/50\n",
      " - 2s - loss: 1.3363 - acc: 0.6500\n",
      "Epoch 33/50\n",
      " - 2s - loss: 1.1835 - acc: 0.6924\n",
      "Epoch 34/50\n",
      " - 2s - loss: 1.1692 - acc: 0.6928\n",
      "Epoch 35/50\n",
      " - 2s - loss: 1.0946 - acc: 0.7054\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.9008 - acc: 0.7652\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.9155 - acc: 0.7616\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.6189 - acc: 0.8626\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.5576 - acc: 0.8814\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.4992 - acc: 0.8950\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.3787 - acc: 0.9362\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.3453 - acc: 0.9412\n",
      "Epoch 43/50\n",
      " - 3s - loss: 1.3010 - acc: 0.7458\n",
      "Epoch 44/50\n",
      " - 2s - loss: 4.9479 - acc: 0.0146\n",
      "Epoch 45/50\n",
      " - 2s - loss: 4.5401 - acc: 0.0250\n",
      "Epoch 46/50\n",
      " - 2s - loss: 2.8222 - acc: 0.3606\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.5861 - acc: 0.8642\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.4586 - acc: 0.9008\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.2387 - acc: 0.9740\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.1637 - acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd7e10f9e48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add( LSTM(100, input_shape= (X.shape[1], X.shape[2])))\n",
    "model.add( Dense(n_features, activation= 'softmax'))\n",
    "model.compile( loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['acc'])\n",
    "# model.summary()\n",
    "\n",
    "model.fit(X,y, epochs= 50, batch_size= 128, verbose= 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8910874938964843, 0.7400000095367432]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 150 LSTM units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3s - loss: 4.6050 - acc: 0.0104\n",
      "Epoch 2/50\n",
      " - 2s - loss: 4.5887 - acc: 0.0160\n",
      "Epoch 3/50\n",
      " - 2s - loss: 4.5687 - acc: 0.0354\n",
      "Epoch 4/50\n",
      " - 2s - loss: 4.5440 - acc: 0.0356\n",
      "Epoch 5/50\n",
      " - 2s - loss: 4.5015 - acc: 0.0428\n",
      "Epoch 6/50\n",
      " - 2s - loss: 4.4195 - acc: 0.0486\n",
      "Epoch 7/50\n",
      " - 2s - loss: 4.2699 - acc: 0.0598\n",
      "Epoch 8/50\n",
      " - 2s - loss: 4.1018 - acc: 0.0874\n",
      "Epoch 9/50\n",
      " - 2s - loss: 3.9224 - acc: 0.1080\n",
      "Epoch 10/50\n",
      " - 2s - loss: 3.7228 - acc: 0.1362\n",
      "Epoch 11/50\n",
      " - 2s - loss: 3.5380 - acc: 0.1698\n",
      "Epoch 12/50\n",
      " - 2s - loss: 3.3333 - acc: 0.1958\n",
      "Epoch 13/50\n",
      " - 2s - loss: 3.1563 - acc: 0.2244\n",
      "Epoch 14/50\n",
      " - 2s - loss: 2.9375 - acc: 0.2654\n",
      "Epoch 15/50\n",
      " - 2s - loss: 2.7373 - acc: 0.3098\n",
      "Epoch 16/50\n",
      " - 2s - loss: 2.5809 - acc: 0.3446\n",
      "Epoch 17/50\n",
      " - 2s - loss: 2.3486 - acc: 0.3916\n",
      "Epoch 18/50\n",
      " - 3s - loss: 2.1302 - acc: 0.4350\n",
      "Epoch 19/50\n",
      " - 2s - loss: 1.9402 - acc: 0.4852\n",
      "Epoch 20/50\n",
      " - 3s - loss: 1.6428 - acc: 0.5616\n",
      "Epoch 21/50\n",
      " - 2s - loss: 1.6072 - acc: 0.5632\n",
      "Epoch 22/50\n",
      " - 2s - loss: 1.2614 - acc: 0.6640\n",
      "Epoch 23/50\n",
      " - 2s - loss: 1.0384 - acc: 0.7268\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.7623 - acc: 0.8098\n",
      "Epoch 25/50\n",
      " - 2s - loss: 1.0142 - acc: 0.7336\n",
      "Epoch 26/50\n",
      " - 2s - loss: 1.1709 - acc: 0.6836\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.5531 - acc: 0.8752\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.3346 - acc: 0.9474\n",
      "Epoch 29/50\n",
      " - 2s - loss: 1.3050 - acc: 0.7480\n",
      "Epoch 30/50\n",
      " - 2s - loss: 4.6719 - acc: 0.0238\n",
      "Epoch 31/50\n",
      " - 2s - loss: 4.4271 - acc: 0.0400\n",
      "Epoch 32/50\n",
      " - 2s - loss: 3.5198 - acc: 0.1756\n",
      "Epoch 33/50\n",
      " - 2s - loss: 1.1412 - acc: 0.6992\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.4770 - acc: 0.8974\n",
      "Epoch 35/50\n",
      " - 2s - loss: 0.2150 - acc: 0.9748\n",
      "Epoch 36/50\n",
      " - 2s - loss: 0.1491 - acc: 0.9884\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.0832 - acc: 0.9982\n",
      "Epoch 38/50\n",
      " - 2s - loss: 0.0600 - acc: 0.9990\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.0494 - acc: 0.9996\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.0391 - acc: 0.9998\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.0280 - acc: 1.0000\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.0247 - acc: 1.0000\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 45/50\n",
      " - 2s - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 46/50\n",
      " - 2s - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 48/50\n",
      " - 2s - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 49/50\n",
      " - 2s - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.0123 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd7e0de1d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add( LSTM(150, input_shape= (X.shape[1], X.shape[2])))\n",
    "model.add( Dense(n_features, activation= 'softmax'))\n",
    "model.compile( loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['acc'])\n",
    "# model.summary()\n",
    "\n",
    "model.fit(X,y, epochs= 50, batch_size= 128, verbose= 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13788420885801314, 0.9700000286102295]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with 150 units sounds to be the best. Let's give it more test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 534.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 335us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13183870311379434, 0.970300018787384]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_10000, y_test_10000= list(), list()\n",
    "for i in tqdm(range(10000)):\n",
    "    X_temp, y_temp= generate_example(length, n_features, out_index)\n",
    "    X_test_10000.append(X_temp)\n",
    "    y_test_10000.append(y_temp)\n",
    "\n",
    "X_test_10000= np.concatenate(X_test_10000)\n",
    "y_test_10000= np.concatenate(y_test_10000)\n",
    "\n",
    "yhat = model.predict(X_test_10000)\n",
    "#print('Sequence: %s' % [one_hot_decode(x) for x in X_test])\n",
    "#print('Expected: %s' % one_hot_decode(y_test))\n",
    "#print('Predicted: %s' % one_hot_decode(yhat))\n",
    "\n",
    "model.evaluate(X_test_10000, y_test_10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The test accuracy even got better: 94 %\n",
    "### Wo we built a model which can memorize a certain index of a sequence with 94 % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laerning the Alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to try different configurations of LSTM in order to predict the alphabet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed= 27\n",
    "np.random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet= \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# creat mapping of characters to integers and reverse\n",
    "char_to_int= dict ( [ (el, i) for i, el in enumerate(alphabet)] )\n",
    "int_to_char= dict( [ (i, el) for i, el in enumerate(alphabet)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "\n",
    "\"\"\"\n",
    "example:\n",
    "When the seq_length= 2 \n",
    "\n",
    "seq_in  -> ['B', 'C']\n",
    "seq_out -> ['D']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "seq_length= 1\n",
    "dataX= []\n",
    "dataY= []\n",
    "for i in range( 0, len(alphabet) - seq_length, 1):\n",
    "    seq_in= alphabet[i: i+seq_length]\n",
    "    seq_out= alphabet[i+seq_length]\n",
    "    dataX.append( [char_to_int[char] for char in seq_in] )\n",
    "    dataY.append( char_to_int[seq_out] )\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the format of the input X into [samples, timesteps, features]\n",
    "X= np.reshape( dataX, ( len(dataX), seq_length, 1) )\n",
    "\n",
    "# normalize X with min_max scaler\n",
    "X = X / len(alphabet)\n",
    "\n",
    "# one-hot encode the output as well\n",
    "y= np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25, 1, 1)\n",
      "y shape: (25, 26)\n"
     ]
    }
   ],
   "source": [
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 1s - loss: 3.2667 - accuracy: 0.0400\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2588 - accuracy: 0.0400\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2557 - accuracy: 0.0400\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2533 - accuracy: 0.0400\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2502 - accuracy: 0.0400\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2475 - accuracy: 0.0000e+00\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2444 - accuracy: 0.0400\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2420 - accuracy: 0.0000e+00\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.2383 - accuracy: 0.0000e+00\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.2354 - accuracy: 0.0400\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.2320 - accuracy: 0.0000e+00\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.2284 - accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.2245 - accuracy: 0.0400\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.2205 - accuracy: 0.0000e+00\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.2164 - accuracy: 0.0400\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.2117 - accuracy: 0.0000e+00\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.2067 - accuracy: 0.0800\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.2017 - accuracy: 0.0800\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.1959 - accuracy: 0.0400\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.1904 - accuracy: 0.0400\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.1853 - accuracy: 0.0400\n",
      "Epoch 22/500\n",
      " - 0s - loss: 3.1781 - accuracy: 0.0400\n",
      "Epoch 23/500\n",
      " - 0s - loss: 3.1716 - accuracy: 0.0400\n",
      "Epoch 24/500\n",
      " - 0s - loss: 3.1641 - accuracy: 0.0400\n",
      "Epoch 25/500\n",
      " - 0s - loss: 3.1566 - accuracy: 0.0800\n",
      "Epoch 26/500\n",
      " - 0s - loss: 3.1491 - accuracy: 0.0800\n",
      "Epoch 27/500\n",
      " - 0s - loss: 3.1416 - accuracy: 0.0800\n",
      "Epoch 28/500\n",
      " - 0s - loss: 3.1335 - accuracy: 0.0800\n",
      "Epoch 29/500\n",
      " - 0s - loss: 3.1243 - accuracy: 0.0800\n",
      "Epoch 30/500\n",
      " - 0s - loss: 3.1154 - accuracy: 0.0400\n",
      "Epoch 31/500\n",
      " - 0s - loss: 3.1064 - accuracy: 0.0800\n",
      "Epoch 32/500\n",
      " - 0s - loss: 3.0951 - accuracy: 0.0800\n",
      "Epoch 33/500\n",
      " - 0s - loss: 3.0874 - accuracy: 0.0800\n",
      "Epoch 34/500\n",
      " - 0s - loss: 3.0754 - accuracy: 0.1200\n",
      "Epoch 35/500\n",
      " - 0s - loss: 3.0644 - accuracy: 0.0800\n",
      "Epoch 36/500\n",
      " - 0s - loss: 3.0536 - accuracy: 0.0800\n",
      "Epoch 37/500\n",
      " - 0s - loss: 3.0429 - accuracy: 0.0800\n",
      "Epoch 38/500\n",
      " - 0s - loss: 3.0295 - accuracy: 0.0800\n",
      "Epoch 39/500\n",
      " - 0s - loss: 3.0192 - accuracy: 0.0800\n",
      "Epoch 40/500\n",
      " - 0s - loss: 3.0052 - accuracy: 0.1200\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.9948 - accuracy: 0.0800\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.9805 - accuracy: 0.1600\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.9667 - accuracy: 0.1200\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.9546 - accuracy: 0.1200\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.9419 - accuracy: 0.0800\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.9275 - accuracy: 0.0400\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.9153 - accuracy: 0.0400\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.9015 - accuracy: 0.0800\n",
      "Epoch 49/500\n",
      " - 0s - loss: 2.8892 - accuracy: 0.0800\n",
      "Epoch 50/500\n",
      " - 0s - loss: 2.8762 - accuracy: 0.1200\n",
      "Epoch 51/500\n",
      " - 0s - loss: 2.8627 - accuracy: 0.0800\n",
      "Epoch 52/500\n",
      " - 0s - loss: 2.8512 - accuracy: 0.1200\n",
      "Epoch 53/500\n",
      " - 0s - loss: 2.8385 - accuracy: 0.0400\n",
      "Epoch 54/500\n",
      " - 0s - loss: 2.8260 - accuracy: 0.0800\n",
      "Epoch 55/500\n",
      " - 0s - loss: 2.8137 - accuracy: 0.0400\n",
      "Epoch 56/500\n",
      " - 0s - loss: 2.8022 - accuracy: 0.0800\n",
      "Epoch 57/500\n",
      " - 0s - loss: 2.7910 - accuracy: 0.0800\n",
      "Epoch 58/500\n",
      " - 0s - loss: 2.7798 - accuracy: 0.0800\n",
      "Epoch 59/500\n",
      " - 0s - loss: 2.7682 - accuracy: 0.0400\n",
      "Epoch 60/500\n",
      " - 0s - loss: 2.7586 - accuracy: 0.0400\n",
      "Epoch 61/500\n",
      " - 0s - loss: 2.7478 - accuracy: 0.0000e+00\n",
      "Epoch 62/500\n",
      " - 0s - loss: 2.7396 - accuracy: 0.0400\n",
      "Epoch 63/500\n",
      " - 0s - loss: 2.7279 - accuracy: 0.0800\n",
      "Epoch 64/500\n",
      " - 0s - loss: 2.7193 - accuracy: 0.0800\n",
      "Epoch 65/500\n",
      " - 0s - loss: 2.7096 - accuracy: 0.0400\n",
      "Epoch 66/500\n",
      " - 0s - loss: 2.7027 - accuracy: 0.0800\n",
      "Epoch 67/500\n",
      " - 0s - loss: 2.6935 - accuracy: 0.0400\n",
      "Epoch 68/500\n",
      " - 0s - loss: 2.6847 - accuracy: 0.0800\n",
      "Epoch 69/500\n",
      " - 0s - loss: 2.6765 - accuracy: 0.0400\n",
      "Epoch 70/500\n",
      " - 0s - loss: 2.6693 - accuracy: 0.0800\n",
      "Epoch 71/500\n",
      " - 0s - loss: 2.6607 - accuracy: 0.0400\n",
      "Epoch 72/500\n",
      " - 0s - loss: 2.6536 - accuracy: 0.0800\n",
      "Epoch 73/500\n",
      " - 0s - loss: 2.6463 - accuracy: 0.0400\n",
      "Epoch 74/500\n",
      " - 0s - loss: 2.6386 - accuracy: 0.0400\n",
      "Epoch 75/500\n",
      " - 0s - loss: 2.6331 - accuracy: 0.0800\n",
      "Epoch 76/500\n",
      " - 0s - loss: 2.6253 - accuracy: 0.0800\n",
      "Epoch 77/500\n",
      " - 0s - loss: 2.6184 - accuracy: 0.0800\n",
      "Epoch 78/500\n",
      " - 0s - loss: 2.6121 - accuracy: 0.1200\n",
      "Epoch 79/500\n",
      " - 0s - loss: 2.6068 - accuracy: 0.1200\n",
      "Epoch 80/500\n",
      " - 0s - loss: 2.5996 - accuracy: 0.1200\n",
      "Epoch 81/500\n",
      " - 0s - loss: 2.5932 - accuracy: 0.0800\n",
      "Epoch 82/500\n",
      " - 0s - loss: 2.5870 - accuracy: 0.1200\n",
      "Epoch 83/500\n",
      " - 0s - loss: 2.5819 - accuracy: 0.0800\n",
      "Epoch 84/500\n",
      " - 0s - loss: 2.5760 - accuracy: 0.0400\n",
      "Epoch 85/500\n",
      " - 0s - loss: 2.5701 - accuracy: 0.1600\n",
      "Epoch 86/500\n",
      " - 0s - loss: 2.5645 - accuracy: 0.0400\n",
      "Epoch 87/500\n",
      " - 0s - loss: 2.5588 - accuracy: 0.0800\n",
      "Epoch 88/500\n",
      " - 0s - loss: 2.5523 - accuracy: 0.0800\n",
      "Epoch 89/500\n",
      " - 0s - loss: 2.5478 - accuracy: 0.1200\n",
      "Epoch 90/500\n",
      " - 0s - loss: 2.5426 - accuracy: 0.0400\n",
      "Epoch 91/500\n",
      " - 0s - loss: 2.5370 - accuracy: 0.1200\n",
      "Epoch 92/500\n",
      " - 0s - loss: 2.5323 - accuracy: 0.0400\n",
      "Epoch 93/500\n",
      " - 0s - loss: 2.5264 - accuracy: 0.1200\n",
      "Epoch 94/500\n",
      " - 0s - loss: 2.5210 - accuracy: 0.1200\n",
      "Epoch 95/500\n",
      " - 0s - loss: 2.5156 - accuracy: 0.0800\n",
      "Epoch 96/500\n",
      " - 0s - loss: 2.5120 - accuracy: 0.0800\n",
      "Epoch 97/500\n",
      " - 0s - loss: 2.5058 - accuracy: 0.1200\n",
      "Epoch 98/500\n",
      " - 0s - loss: 2.5018 - accuracy: 0.0800\n",
      "Epoch 99/500\n",
      " - 0s - loss: 2.4958 - accuracy: 0.0800\n",
      "Epoch 100/500\n",
      " - 0s - loss: 2.4919 - accuracy: 0.0800\n",
      "Epoch 101/500\n",
      " - 0s - loss: 2.4859 - accuracy: 0.0800\n",
      "Epoch 102/500\n",
      " - 0s - loss: 2.4816 - accuracy: 0.1600\n",
      "Epoch 103/500\n",
      " - 0s - loss: 2.4768 - accuracy: 0.1200\n",
      "Epoch 104/500\n",
      " - 0s - loss: 2.4707 - accuracy: 0.1600\n",
      "Epoch 105/500\n",
      " - 0s - loss: 2.4673 - accuracy: 0.1600\n",
      "Epoch 106/500\n",
      " - 0s - loss: 2.4624 - accuracy: 0.1600\n",
      "Epoch 107/500\n",
      " - 0s - loss: 2.4569 - accuracy: 0.1600\n",
      "Epoch 108/500\n",
      " - 0s - loss: 2.4544 - accuracy: 0.1600\n",
      "Epoch 109/500\n",
      " - 0s - loss: 2.4479 - accuracy: 0.1600\n",
      "Epoch 110/500\n",
      " - 0s - loss: 2.4446 - accuracy: 0.1200\n",
      "Epoch 111/500\n",
      " - 0s - loss: 2.4386 - accuracy: 0.1600\n",
      "Epoch 112/500\n",
      " - 0s - loss: 2.4364 - accuracy: 0.1600\n",
      "Epoch 113/500\n",
      " - 0s - loss: 2.4306 - accuracy: 0.1200\n",
      "Epoch 114/500\n",
      " - 0s - loss: 2.4257 - accuracy: 0.1600\n",
      "Epoch 115/500\n",
      " - 0s - loss: 2.4215 - accuracy: 0.1600\n",
      "Epoch 116/500\n",
      " - 0s - loss: 2.4184 - accuracy: 0.1600\n",
      "Epoch 117/500\n",
      " - 0s - loss: 2.4139 - accuracy: 0.1600\n",
      "Epoch 118/500\n",
      " - 0s - loss: 2.4081 - accuracy: 0.1600\n",
      "Epoch 119/500\n",
      " - 0s - loss: 2.4046 - accuracy: 0.1200\n",
      "Epoch 120/500\n",
      " - 0s - loss: 2.4007 - accuracy: 0.1200\n",
      "Epoch 121/500\n",
      " - 0s - loss: 2.3955 - accuracy: 0.1600\n",
      "Epoch 122/500\n",
      " - 0s - loss: 2.3915 - accuracy: 0.1600\n",
      "Epoch 123/500\n",
      " - 0s - loss: 2.3879 - accuracy: 0.1600\n",
      "Epoch 124/500\n",
      " - 0s - loss: 2.3835 - accuracy: 0.1600\n",
      "Epoch 125/500\n",
      " - 0s - loss: 2.3802 - accuracy: 0.1600\n",
      "Epoch 126/500\n",
      " - 0s - loss: 2.3762 - accuracy: 0.1600\n",
      "Epoch 127/500\n",
      " - 0s - loss: 2.3718 - accuracy: 0.1600\n",
      "Epoch 128/500\n",
      " - 0s - loss: 2.3677 - accuracy: 0.1600\n",
      "Epoch 129/500\n",
      " - 0s - loss: 2.3648 - accuracy: 0.1600\n",
      "Epoch 130/500\n",
      " - 0s - loss: 2.3607 - accuracy: 0.1600\n",
      "Epoch 131/500\n",
      " - 0s - loss: 2.3562 - accuracy: 0.1600\n",
      "Epoch 132/500\n",
      " - 0s - loss: 2.3535 - accuracy: 0.2000\n",
      "Epoch 133/500\n",
      " - 0s - loss: 2.3485 - accuracy: 0.2000\n",
      "Epoch 134/500\n",
      " - 0s - loss: 2.3451 - accuracy: 0.2000\n",
      "Epoch 135/500\n",
      " - 0s - loss: 2.3401 - accuracy: 0.1600\n",
      "Epoch 136/500\n",
      " - 0s - loss: 2.3382 - accuracy: 0.1600\n",
      "Epoch 137/500\n",
      " - 0s - loss: 2.3341 - accuracy: 0.2000\n",
      "Epoch 138/500\n",
      " - 0s - loss: 2.3310 - accuracy: 0.2400\n",
      "Epoch 139/500\n",
      " - 0s - loss: 2.3264 - accuracy: 0.2400\n",
      "Epoch 140/500\n",
      " - 0s - loss: 2.3230 - accuracy: 0.2000\n",
      "Epoch 141/500\n",
      " - 0s - loss: 2.3200 - accuracy: 0.2400\n",
      "Epoch 142/500\n",
      " - 0s - loss: 2.3150 - accuracy: 0.2800\n",
      "Epoch 143/500\n",
      " - 0s - loss: 2.3121 - accuracy: 0.2400\n",
      "Epoch 144/500\n",
      " - 0s - loss: 2.3078 - accuracy: 0.2400\n",
      "Epoch 145/500\n",
      " - 0s - loss: 2.3064 - accuracy: 0.2000\n",
      "Epoch 146/500\n",
      " - 0s - loss: 2.3019 - accuracy: 0.2800\n",
      "Epoch 147/500\n",
      " - 0s - loss: 2.2992 - accuracy: 0.2000\n",
      "Epoch 148/500\n",
      " - 0s - loss: 2.2937 - accuracy: 0.2400\n",
      "Epoch 149/500\n",
      " - 0s - loss: 2.2899 - accuracy: 0.2000\n",
      "Epoch 150/500\n",
      " - 0s - loss: 2.2881 - accuracy: 0.2800\n",
      "Epoch 151/500\n",
      " - 0s - loss: 2.2843 - accuracy: 0.2000\n",
      "Epoch 152/500\n",
      " - 0s - loss: 2.2813 - accuracy: 0.2400\n",
      "Epoch 153/500\n",
      " - 0s - loss: 2.2780 - accuracy: 0.2800\n",
      "Epoch 154/500\n",
      " - 0s - loss: 2.2745 - accuracy: 0.2000\n",
      "Epoch 155/500\n",
      " - 0s - loss: 2.2726 - accuracy: 0.3200\n",
      "Epoch 156/500\n",
      " - 0s - loss: 2.2676 - accuracy: 0.2800\n",
      "Epoch 157/500\n",
      " - 0s - loss: 2.2665 - accuracy: 0.3200\n",
      "Epoch 158/500\n",
      " - 0s - loss: 2.2617 - accuracy: 0.2800\n",
      "Epoch 159/500\n",
      " - 0s - loss: 2.2596 - accuracy: 0.2800\n",
      "Epoch 160/500\n",
      " - 0s - loss: 2.2555 - accuracy: 0.3200\n",
      "Epoch 161/500\n",
      " - 0s - loss: 2.2536 - accuracy: 0.2800\n",
      "Epoch 162/500\n",
      " - 0s - loss: 2.2482 - accuracy: 0.2800\n",
      "Epoch 163/500\n",
      " - 0s - loss: 2.2471 - accuracy: 0.3200\n",
      "Epoch 164/500\n",
      " - 0s - loss: 2.2430 - accuracy: 0.2800\n",
      "Epoch 165/500\n",
      " - 0s - loss: 2.2399 - accuracy: 0.3200\n",
      "Epoch 166/500\n",
      " - 0s - loss: 2.2369 - accuracy: 0.2000\n",
      "Epoch 167/500\n",
      " - 0s - loss: 2.2351 - accuracy: 0.2800\n",
      "Epoch 168/500\n",
      " - 0s - loss: 2.2309 - accuracy: 0.3200\n",
      "Epoch 169/500\n",
      " - 0s - loss: 2.2286 - accuracy: 0.2400\n",
      "Epoch 170/500\n",
      " - 0s - loss: 2.2259 - accuracy: 0.2400\n",
      "Epoch 171/500\n",
      " - 0s - loss: 2.2227 - accuracy: 0.2800\n",
      "Epoch 172/500\n",
      " - 0s - loss: 2.2197 - accuracy: 0.3200\n",
      "Epoch 173/500\n",
      " - 0s - loss: 2.2182 - accuracy: 0.3200\n",
      "Epoch 174/500\n",
      " - 0s - loss: 2.2133 - accuracy: 0.3200\n",
      "Epoch 175/500\n",
      " - 0s - loss: 2.2110 - accuracy: 0.2000\n",
      "Epoch 176/500\n",
      " - 0s - loss: 2.2086 - accuracy: 0.3200\n",
      "Epoch 177/500\n",
      " - 0s - loss: 2.2054 - accuracy: 0.3200\n",
      "Epoch 178/500\n",
      " - 0s - loss: 2.2028 - accuracy: 0.3200\n",
      "Epoch 179/500\n",
      " - 0s - loss: 2.1989 - accuracy: 0.2800\n",
      "Epoch 180/500\n",
      " - 0s - loss: 2.1963 - accuracy: 0.3600\n",
      "Epoch 181/500\n",
      " - 0s - loss: 2.1954 - accuracy: 0.3600\n",
      "Epoch 182/500\n",
      " - 0s - loss: 2.1911 - accuracy: 0.3600\n",
      "Epoch 183/500\n",
      " - 0s - loss: 2.1896 - accuracy: 0.3600\n",
      "Epoch 184/500\n",
      " - 0s - loss: 2.1879 - accuracy: 0.3200\n",
      "Epoch 185/500\n",
      " - 0s - loss: 2.1844 - accuracy: 0.3200\n",
      "Epoch 186/500\n",
      " - 0s - loss: 2.1820 - accuracy: 0.3600\n",
      "Epoch 187/500\n",
      " - 0s - loss: 2.1763 - accuracy: 0.4000\n",
      "Epoch 188/500\n",
      " - 0s - loss: 2.1746 - accuracy: 0.4000\n",
      "Epoch 189/500\n",
      " - 0s - loss: 2.1717 - accuracy: 0.3600\n",
      "Epoch 190/500\n",
      " - 0s - loss: 2.1711 - accuracy: 0.3200\n",
      "Epoch 191/500\n",
      " - 0s - loss: 2.1679 - accuracy: 0.3200\n",
      "Epoch 192/500\n",
      " - 0s - loss: 2.1645 - accuracy: 0.3600\n",
      "Epoch 193/500\n",
      " - 0s - loss: 2.1629 - accuracy: 0.3200\n",
      "Epoch 194/500\n",
      " - 0s - loss: 2.1604 - accuracy: 0.3600\n",
      "Epoch 195/500\n",
      " - 0s - loss: 2.1574 - accuracy: 0.3600\n",
      "Epoch 196/500\n",
      " - 0s - loss: 2.1552 - accuracy: 0.4000\n",
      "Epoch 197/500\n",
      " - 0s - loss: 2.1526 - accuracy: 0.3600\n",
      "Epoch 198/500\n",
      " - 0s - loss: 2.1495 - accuracy: 0.3600\n",
      "Epoch 199/500\n",
      " - 0s - loss: 2.1464 - accuracy: 0.4000\n",
      "Epoch 200/500\n",
      " - 0s - loss: 2.1452 - accuracy: 0.3200\n",
      "Epoch 201/500\n",
      " - 0s - loss: 2.1430 - accuracy: 0.3600\n",
      "Epoch 202/500\n",
      " - 0s - loss: 2.1412 - accuracy: 0.3600\n",
      "Epoch 203/500\n",
      " - 0s - loss: 2.1393 - accuracy: 0.4800\n",
      "Epoch 204/500\n",
      " - 0s - loss: 2.1356 - accuracy: 0.4400\n",
      "Epoch 205/500\n",
      " - 0s - loss: 2.1346 - accuracy: 0.2400\n",
      "Epoch 206/500\n",
      " - 0s - loss: 2.1299 - accuracy: 0.3200\n",
      "Epoch 207/500\n",
      " - 0s - loss: 2.1272 - accuracy: 0.3200\n",
      "Epoch 208/500\n",
      " - 0s - loss: 2.1245 - accuracy: 0.4000\n",
      "Epoch 209/500\n",
      " - 0s - loss: 2.1238 - accuracy: 0.3600\n",
      "Epoch 210/500\n",
      " - 0s - loss: 2.1211 - accuracy: 0.3600\n",
      "Epoch 211/500\n",
      " - 0s - loss: 2.1195 - accuracy: 0.4000\n",
      "Epoch 212/500\n",
      " - 0s - loss: 2.1170 - accuracy: 0.4000\n",
      "Epoch 213/500\n",
      " - 0s - loss: 2.1145 - accuracy: 0.4000\n",
      "Epoch 214/500\n",
      " - 0s - loss: 2.1105 - accuracy: 0.4400\n",
      "Epoch 215/500\n",
      " - 0s - loss: 2.1101 - accuracy: 0.3200\n",
      "Epoch 216/500\n",
      " - 0s - loss: 2.1082 - accuracy: 0.4800\n",
      "Epoch 217/500\n",
      " - 0s - loss: 2.1056 - accuracy: 0.4000\n",
      "Epoch 218/500\n",
      " - 0s - loss: 2.1028 - accuracy: 0.3600\n",
      "Epoch 219/500\n",
      " - 0s - loss: 2.0998 - accuracy: 0.2800\n",
      "Epoch 220/500\n",
      " - 0s - loss: 2.0991 - accuracy: 0.5200\n",
      "Epoch 221/500\n",
      " - 0s - loss: 2.0970 - accuracy: 0.4000\n",
      "Epoch 222/500\n",
      " - 0s - loss: 2.0935 - accuracy: 0.4000\n",
      "Epoch 223/500\n",
      " - 0s - loss: 2.0908 - accuracy: 0.4000\n",
      "Epoch 224/500\n",
      " - 0s - loss: 2.0892 - accuracy: 0.5600\n",
      "Epoch 225/500\n",
      " - 0s - loss: 2.0873 - accuracy: 0.4400\n",
      "Epoch 226/500\n",
      " - 0s - loss: 2.0841 - accuracy: 0.4400\n",
      "Epoch 227/500\n",
      " - 0s - loss: 2.0835 - accuracy: 0.4400\n",
      "Epoch 228/500\n",
      " - 0s - loss: 2.0812 - accuracy: 0.3600\n",
      "Epoch 229/500\n",
      " - 0s - loss: 2.0768 - accuracy: 0.4400\n",
      "Epoch 230/500\n",
      " - 0s - loss: 2.0766 - accuracy: 0.5600\n",
      "Epoch 231/500\n",
      " - 0s - loss: 2.0751 - accuracy: 0.4000\n",
      "Epoch 232/500\n",
      " - 0s - loss: 2.0723 - accuracy: 0.5200\n",
      "Epoch 233/500\n",
      " - 0s - loss: 2.0713 - accuracy: 0.4800\n",
      "Epoch 234/500\n",
      " - 0s - loss: 2.0682 - accuracy: 0.4800\n",
      "Epoch 235/500\n",
      " - 0s - loss: 2.0673 - accuracy: 0.4400\n",
      "Epoch 236/500\n",
      " - 0s - loss: 2.0620 - accuracy: 0.4400\n",
      "Epoch 237/500\n",
      " - 0s - loss: 2.0619 - accuracy: 0.4800\n",
      "Epoch 238/500\n",
      " - 0s - loss: 2.0591 - accuracy: 0.5200\n",
      "Epoch 239/500\n",
      " - 0s - loss: 2.0569 - accuracy: 0.5200\n",
      "Epoch 240/500\n",
      " - 0s - loss: 2.0567 - accuracy: 0.4800\n",
      "Epoch 241/500\n",
      " - 0s - loss: 2.0538 - accuracy: 0.4800\n",
      "Epoch 242/500\n",
      " - 0s - loss: 2.0526 - accuracy: 0.4400\n",
      "Epoch 243/500\n",
      " - 0s - loss: 2.0495 - accuracy: 0.4400\n",
      "Epoch 244/500\n",
      " - 0s - loss: 2.0478 - accuracy: 0.4800\n",
      "Epoch 245/500\n",
      " - 0s - loss: 2.0455 - accuracy: 0.4800\n",
      "Epoch 246/500\n",
      " - 0s - loss: 2.0445 - accuracy: 0.4800\n",
      "Epoch 247/500\n",
      " - 0s - loss: 2.0419 - accuracy: 0.5200\n",
      "Epoch 248/500\n",
      " - 0s - loss: 2.0395 - accuracy: 0.4800\n",
      "Epoch 249/500\n",
      " - 0s - loss: 2.0379 - accuracy: 0.5200\n",
      "Epoch 250/500\n",
      " - 0s - loss: 2.0370 - accuracy: 0.4400\n",
      "Epoch 251/500\n",
      " - 0s - loss: 2.0352 - accuracy: 0.4000\n",
      "Epoch 252/500\n",
      " - 0s - loss: 2.0330 - accuracy: 0.4800\n",
      "Epoch 253/500\n",
      " - 0s - loss: 2.0303 - accuracy: 0.4800\n",
      "Epoch 254/500\n",
      " - 0s - loss: 2.0300 - accuracy: 0.5600\n",
      "Epoch 255/500\n",
      " - 0s - loss: 2.0289 - accuracy: 0.5200\n",
      "Epoch 256/500\n",
      " - 0s - loss: 2.0251 - accuracy: 0.4400\n",
      "Epoch 257/500\n",
      " - 0s - loss: 2.0234 - accuracy: 0.4400\n",
      "Epoch 258/500\n",
      " - 0s - loss: 2.0208 - accuracy: 0.5200\n",
      "Epoch 259/500\n",
      " - 0s - loss: 2.0200 - accuracy: 0.4800\n",
      "Epoch 260/500\n",
      " - 0s - loss: 2.0184 - accuracy: 0.4800\n",
      "Epoch 261/500\n",
      " - 0s - loss: 2.0141 - accuracy: 0.5200\n",
      "Epoch 262/500\n",
      " - 0s - loss: 2.0138 - accuracy: 0.4800\n",
      "Epoch 263/500\n",
      " - 0s - loss: 2.0129 - accuracy: 0.4800\n",
      "Epoch 264/500\n",
      " - 0s - loss: 2.0101 - accuracy: 0.4000\n",
      "Epoch 265/500\n",
      " - 0s - loss: 2.0078 - accuracy: 0.4400\n",
      "Epoch 266/500\n",
      " - 0s - loss: 2.0082 - accuracy: 0.4800\n",
      "Epoch 267/500\n",
      " - 0s - loss: 2.0043 - accuracy: 0.4800\n",
      "Epoch 268/500\n",
      " - 0s - loss: 2.0033 - accuracy: 0.6000\n",
      "Epoch 269/500\n",
      " - 0s - loss: 2.0015 - accuracy: 0.4800\n",
      "Epoch 270/500\n",
      " - 0s - loss: 1.9998 - accuracy: 0.5200\n",
      "Epoch 271/500\n",
      " - 0s - loss: 1.9973 - accuracy: 0.5600\n",
      "Epoch 272/500\n",
      " - 0s - loss: 1.9940 - accuracy: 0.5600\n",
      "Epoch 273/500\n",
      " - 0s - loss: 1.9945 - accuracy: 0.5200\n",
      "Epoch 274/500\n",
      " - 0s - loss: 1.9920 - accuracy: 0.4800\n",
      "Epoch 275/500\n",
      " - 0s - loss: 1.9913 - accuracy: 0.5200\n",
      "Epoch 276/500\n",
      " - 0s - loss: 1.9884 - accuracy: 0.4400\n",
      "Epoch 277/500\n",
      " - 0s - loss: 1.9853 - accuracy: 0.5200\n",
      "Epoch 278/500\n",
      " - 0s - loss: 1.9843 - accuracy: 0.4400\n",
      "Epoch 279/500\n",
      " - 0s - loss: 1.9834 - accuracy: 0.5600\n",
      "Epoch 280/500\n",
      " - 0s - loss: 1.9821 - accuracy: 0.5600\n",
      "Epoch 281/500\n",
      " - 0s - loss: 1.9802 - accuracy: 0.4800\n",
      "Epoch 282/500\n",
      " - 0s - loss: 1.9793 - accuracy: 0.5200\n",
      "Epoch 283/500\n",
      " - 0s - loss: 1.9757 - accuracy: 0.5600\n",
      "Epoch 284/500\n",
      " - 0s - loss: 1.9747 - accuracy: 0.5200\n",
      "Epoch 285/500\n",
      " - 0s - loss: 1.9722 - accuracy: 0.5200\n",
      "Epoch 286/500\n",
      " - 0s - loss: 1.9719 - accuracy: 0.5600\n",
      "Epoch 287/500\n",
      " - 0s - loss: 1.9691 - accuracy: 0.5600\n",
      "Epoch 288/500\n",
      " - 0s - loss: 1.9691 - accuracy: 0.4800\n",
      "Epoch 289/500\n",
      " - 0s - loss: 1.9662 - accuracy: 0.6400\n",
      "Epoch 290/500\n",
      " - 0s - loss: 1.9646 - accuracy: 0.4400\n",
      "Epoch 291/500\n",
      " - 0s - loss: 1.9626 - accuracy: 0.5200\n",
      "Epoch 292/500\n",
      " - 0s - loss: 1.9641 - accuracy: 0.5600\n",
      "Epoch 293/500\n",
      " - 0s - loss: 1.9612 - accuracy: 0.5600\n",
      "Epoch 294/500\n",
      " - 0s - loss: 1.9581 - accuracy: 0.4800\n",
      "Epoch 295/500\n",
      " - 0s - loss: 1.9567 - accuracy: 0.4800\n",
      "Epoch 296/500\n",
      " - 0s - loss: 1.9548 - accuracy: 0.6000\n",
      "Epoch 297/500\n",
      " - 0s - loss: 1.9526 - accuracy: 0.7200\n",
      "Epoch 298/500\n",
      " - 0s - loss: 1.9526 - accuracy: 0.5600\n",
      "Epoch 299/500\n",
      " - 0s - loss: 1.9499 - accuracy: 0.6400\n",
      "Epoch 300/500\n",
      " - 0s - loss: 1.9503 - accuracy: 0.4800\n",
      "Epoch 301/500\n",
      " - 0s - loss: 1.9466 - accuracy: 0.6000\n",
      "Epoch 302/500\n",
      " - 0s - loss: 1.9466 - accuracy: 0.6400\n",
      "Epoch 303/500\n",
      " - 0s - loss: 1.9449 - accuracy: 0.5600\n",
      "Epoch 304/500\n",
      " - 0s - loss: 1.9420 - accuracy: 0.5200\n",
      "Epoch 305/500\n",
      " - 0s - loss: 1.9409 - accuracy: 0.6000\n",
      "Epoch 306/500\n",
      " - 0s - loss: 1.9409 - accuracy: 0.5600\n",
      "Epoch 307/500\n",
      " - 0s - loss: 1.9381 - accuracy: 0.5600\n",
      "Epoch 308/500\n",
      " - 0s - loss: 1.9376 - accuracy: 0.5600\n",
      "Epoch 309/500\n",
      " - 0s - loss: 1.9339 - accuracy: 0.5200\n",
      "Epoch 310/500\n",
      " - 0s - loss: 1.9342 - accuracy: 0.5600\n",
      "Epoch 311/500\n",
      " - 0s - loss: 1.9343 - accuracy: 0.6000\n",
      "Epoch 312/500\n",
      " - 0s - loss: 1.9315 - accuracy: 0.6000\n",
      "Epoch 313/500\n",
      " - 0s - loss: 1.9305 - accuracy: 0.6000\n",
      "Epoch 314/500\n",
      " - 0s - loss: 1.9298 - accuracy: 0.6800\n",
      "Epoch 315/500\n",
      " - 0s - loss: 1.9263 - accuracy: 0.6000\n",
      "Epoch 316/500\n",
      " - 0s - loss: 1.9240 - accuracy: 0.6400\n",
      "Epoch 317/500\n",
      " - 0s - loss: 1.9240 - accuracy: 0.5200\n",
      "Epoch 318/500\n",
      " - 0s - loss: 1.9223 - accuracy: 0.6400\n",
      "Epoch 319/500\n",
      " - 0s - loss: 1.9203 - accuracy: 0.5600\n",
      "Epoch 320/500\n",
      " - 0s - loss: 1.9186 - accuracy: 0.6000\n",
      "Epoch 321/500\n",
      " - 0s - loss: 1.9165 - accuracy: 0.7200\n",
      "Epoch 322/500\n",
      " - 0s - loss: 1.9162 - accuracy: 0.4800\n",
      "Epoch 323/500\n",
      " - 0s - loss: 1.9135 - accuracy: 0.7200\n",
      "Epoch 324/500\n",
      " - 0s - loss: 1.9141 - accuracy: 0.6000\n",
      "Epoch 325/500\n",
      " - 0s - loss: 1.9107 - accuracy: 0.5600\n",
      "Epoch 326/500\n",
      " - 0s - loss: 1.9097 - accuracy: 0.6000\n",
      "Epoch 327/500\n",
      " - 0s - loss: 1.9093 - accuracy: 0.6000\n",
      "Epoch 328/500\n",
      " - 0s - loss: 1.9072 - accuracy: 0.5600\n",
      "Epoch 329/500\n",
      " - 0s - loss: 1.9059 - accuracy: 0.6400\n",
      "Epoch 330/500\n",
      " - 0s - loss: 1.9041 - accuracy: 0.5600\n",
      "Epoch 331/500\n",
      " - 0s - loss: 1.9024 - accuracy: 0.5600\n",
      "Epoch 332/500\n",
      " - 0s - loss: 1.9025 - accuracy: 0.7200\n",
      "Epoch 333/500\n",
      " - 0s - loss: 1.9015 - accuracy: 0.5600\n",
      "Epoch 334/500\n",
      " - 0s - loss: 1.9000 - accuracy: 0.6400\n",
      "Epoch 335/500\n",
      " - 0s - loss: 1.8975 - accuracy: 0.6400\n",
      "Epoch 336/500\n",
      " - 0s - loss: 1.8982 - accuracy: 0.5600\n",
      "Epoch 337/500\n",
      " - 0s - loss: 1.8948 - accuracy: 0.7200\n",
      "Epoch 338/500\n",
      " - 0s - loss: 1.8922 - accuracy: 0.5200\n",
      "Epoch 339/500\n",
      " - 0s - loss: 1.8917 - accuracy: 0.6400\n",
      "Epoch 340/500\n",
      " - 0s - loss: 1.8896 - accuracy: 0.6800\n",
      "Epoch 341/500\n",
      " - 0s - loss: 1.8896 - accuracy: 0.6000\n",
      "Epoch 342/500\n",
      " - 0s - loss: 1.8876 - accuracy: 0.6800\n",
      "Epoch 343/500\n",
      " - 0s - loss: 1.8866 - accuracy: 0.6400\n",
      "Epoch 344/500\n",
      " - 0s - loss: 1.8835 - accuracy: 0.6000\n",
      "Epoch 345/500\n",
      " - 0s - loss: 1.8850 - accuracy: 0.5600\n",
      "Epoch 346/500\n",
      " - 0s - loss: 1.8825 - accuracy: 0.6800\n",
      "Epoch 347/500\n",
      " - 0s - loss: 1.8825 - accuracy: 0.6000\n",
      "Epoch 348/500\n",
      " - 0s - loss: 1.8796 - accuracy: 0.6000\n",
      "Epoch 349/500\n",
      " - 0s - loss: 1.8793 - accuracy: 0.6000\n",
      "Epoch 350/500\n",
      " - 0s - loss: 1.8765 - accuracy: 0.6000\n",
      "Epoch 351/500\n",
      " - 0s - loss: 1.8759 - accuracy: 0.5600\n",
      "Epoch 352/500\n",
      " - 0s - loss: 1.8757 - accuracy: 0.6400\n",
      "Epoch 353/500\n",
      " - 0s - loss: 1.8729 - accuracy: 0.6800\n",
      "Epoch 354/500\n",
      " - 0s - loss: 1.8711 - accuracy: 0.6000\n",
      "Epoch 355/500\n",
      " - 0s - loss: 1.8695 - accuracy: 0.6000\n",
      "Epoch 356/500\n",
      " - 0s - loss: 1.8678 - accuracy: 0.5600\n",
      "Epoch 357/500\n",
      " - 0s - loss: 1.8689 - accuracy: 0.5600\n",
      "Epoch 358/500\n",
      " - 0s - loss: 1.8662 - accuracy: 0.6000\n",
      "Epoch 359/500\n",
      " - 0s - loss: 1.8650 - accuracy: 0.6400\n",
      "Epoch 360/500\n",
      " - 0s - loss: 1.8639 - accuracy: 0.7200\n",
      "Epoch 361/500\n",
      " - 0s - loss: 1.8619 - accuracy: 0.6400\n",
      "Epoch 362/500\n",
      " - 0s - loss: 1.8612 - accuracy: 0.6400\n",
      "Epoch 363/500\n",
      " - 0s - loss: 1.8612 - accuracy: 0.6000\n",
      "Epoch 364/500\n",
      " - 0s - loss: 1.8586 - accuracy: 0.6400\n",
      "Epoch 365/500\n",
      " - 0s - loss: 1.8585 - accuracy: 0.5600\n",
      "Epoch 366/500\n",
      " - 0s - loss: 1.8569 - accuracy: 0.6400\n",
      "Epoch 367/500\n",
      " - 0s - loss: 1.8571 - accuracy: 0.6800\n",
      "Epoch 368/500\n",
      " - 0s - loss: 1.8538 - accuracy: 0.6400\n",
      "Epoch 369/500\n",
      " - 0s - loss: 1.8532 - accuracy: 0.5600\n",
      "Epoch 370/500\n",
      " - 0s - loss: 1.8499 - accuracy: 0.6000\n",
      "Epoch 371/500\n",
      " - 0s - loss: 1.8506 - accuracy: 0.5600\n",
      "Epoch 372/500\n",
      " - 0s - loss: 1.8482 - accuracy: 0.6000\n",
      "Epoch 373/500\n",
      " - 0s - loss: 1.8480 - accuracy: 0.7600\n",
      "Epoch 374/500\n",
      " - 0s - loss: 1.8460 - accuracy: 0.6800\n",
      "Epoch 375/500\n",
      " - 0s - loss: 1.8447 - accuracy: 0.7200\n",
      "Epoch 376/500\n",
      " - 0s - loss: 1.8452 - accuracy: 0.6800\n",
      "Epoch 377/500\n",
      " - 0s - loss: 1.8454 - accuracy: 0.6400\n",
      "Epoch 378/500\n",
      " - 0s - loss: 1.8422 - accuracy: 0.7600\n",
      "Epoch 379/500\n",
      " - 0s - loss: 1.8403 - accuracy: 0.6000\n",
      "Epoch 380/500\n",
      " - 0s - loss: 1.8409 - accuracy: 0.6800\n",
      "Epoch 381/500\n",
      " - 0s - loss: 1.8380 - accuracy: 0.6400\n",
      "Epoch 382/500\n",
      " - 0s - loss: 1.8383 - accuracy: 0.7200\n",
      "Epoch 383/500\n",
      " - 0s - loss: 1.8378 - accuracy: 0.6400\n",
      "Epoch 384/500\n",
      " - 0s - loss: 1.8364 - accuracy: 0.6800\n",
      "Epoch 385/500\n",
      " - 0s - loss: 1.8336 - accuracy: 0.6400\n",
      "Epoch 386/500\n",
      " - 0s - loss: 1.8314 - accuracy: 0.6800\n",
      "Epoch 387/500\n",
      " - 0s - loss: 1.8310 - accuracy: 0.6400\n",
      "Epoch 388/500\n",
      " - 0s - loss: 1.8296 - accuracy: 0.6400\n",
      "Epoch 389/500\n",
      " - 0s - loss: 1.8302 - accuracy: 0.6800\n",
      "Epoch 390/500\n",
      " - 0s - loss: 1.8268 - accuracy: 0.7600\n",
      "Epoch 391/500\n",
      " - 0s - loss: 1.8269 - accuracy: 0.6400\n",
      "Epoch 392/500\n",
      " - 0s - loss: 1.8256 - accuracy: 0.6400\n",
      "Epoch 393/500\n",
      " - 0s - loss: 1.8254 - accuracy: 0.7200\n",
      "Epoch 394/500\n",
      " - 0s - loss: 1.8228 - accuracy: 0.6800\n",
      "Epoch 395/500\n",
      " - 0s - loss: 1.8232 - accuracy: 0.7200\n",
      "Epoch 396/500\n",
      " - 0s - loss: 1.8213 - accuracy: 0.6400\n",
      "Epoch 397/500\n",
      " - 0s - loss: 1.8194 - accuracy: 0.7200\n",
      "Epoch 398/500\n",
      " - 0s - loss: 1.8165 - accuracy: 0.6400\n",
      "Epoch 399/500\n",
      " - 0s - loss: 1.8179 - accuracy: 0.7200\n",
      "Epoch 400/500\n",
      " - 0s - loss: 1.8139 - accuracy: 0.6800\n",
      "Epoch 401/500\n",
      " - 0s - loss: 1.8153 - accuracy: 0.6800\n",
      "Epoch 402/500\n",
      " - 0s - loss: 1.8136 - accuracy: 0.6400\n",
      "Epoch 403/500\n",
      " - 0s - loss: 1.8129 - accuracy: 0.7200\n",
      "Epoch 404/500\n",
      " - 0s - loss: 1.8107 - accuracy: 0.7600\n",
      "Epoch 405/500\n",
      " - 0s - loss: 1.8099 - accuracy: 0.6400\n",
      "Epoch 406/500\n",
      " - 0s - loss: 1.8100 - accuracy: 0.6800\n",
      "Epoch 407/500\n",
      " - 0s - loss: 1.8084 - accuracy: 0.6800\n",
      "Epoch 408/500\n",
      " - 0s - loss: 1.8062 - accuracy: 0.7600\n",
      "Epoch 409/500\n",
      " - 0s - loss: 1.8091 - accuracy: 0.7600\n",
      "Epoch 410/500\n",
      " - 0s - loss: 1.8048 - accuracy: 0.6800\n",
      "Epoch 411/500\n",
      " - 0s - loss: 1.8037 - accuracy: 0.7200\n",
      "Epoch 412/500\n",
      " - 0s - loss: 1.8007 - accuracy: 0.7600\n",
      "Epoch 413/500\n",
      " - 0s - loss: 1.8013 - accuracy: 0.7200\n",
      "Epoch 414/500\n",
      " - 0s - loss: 1.7992 - accuracy: 0.7200\n",
      "Epoch 415/500\n",
      " - 0s - loss: 1.7989 - accuracy: 0.6800\n",
      "Epoch 416/500\n",
      " - 0s - loss: 1.8009 - accuracy: 0.7200\n",
      "Epoch 417/500\n",
      " - 0s - loss: 1.7966 - accuracy: 0.6400\n",
      "Epoch 418/500\n",
      " - 0s - loss: 1.7950 - accuracy: 0.7200\n",
      "Epoch 419/500\n",
      " - 0s - loss: 1.7931 - accuracy: 0.7600\n",
      "Epoch 420/500\n",
      " - 0s - loss: 1.7922 - accuracy: 0.7200\n",
      "Epoch 421/500\n",
      " - 0s - loss: 1.7928 - accuracy: 0.6800\n",
      "Epoch 422/500\n",
      " - 0s - loss: 1.7932 - accuracy: 0.7600\n",
      "Epoch 423/500\n",
      " - 0s - loss: 1.7900 - accuracy: 0.7200\n",
      "Epoch 424/500\n",
      " - 0s - loss: 1.7886 - accuracy: 0.6800\n",
      "Epoch 425/500\n",
      " - 0s - loss: 1.7902 - accuracy: 0.7600\n",
      "Epoch 426/500\n",
      " - 0s - loss: 1.7884 - accuracy: 0.7600\n",
      "Epoch 427/500\n",
      " - 0s - loss: 1.7860 - accuracy: 0.8000\n",
      "Epoch 428/500\n",
      " - 0s - loss: 1.7852 - accuracy: 0.6400\n",
      "Epoch 429/500\n",
      " - 0s - loss: 1.7837 - accuracy: 0.6800\n",
      "Epoch 430/500\n",
      " - 0s - loss: 1.7827 - accuracy: 0.7200\n",
      "Epoch 431/500\n",
      " - 0s - loss: 1.7835 - accuracy: 0.6400\n",
      "Epoch 432/500\n",
      " - 0s - loss: 1.7812 - accuracy: 0.6400\n",
      "Epoch 433/500\n",
      " - 0s - loss: 1.7795 - accuracy: 0.6800\n",
      "Epoch 434/500\n",
      " - 0s - loss: 1.7778 - accuracy: 0.8000\n",
      "Epoch 435/500\n",
      " - 0s - loss: 1.7794 - accuracy: 0.7600\n",
      "Epoch 436/500\n",
      " - 0s - loss: 1.7776 - accuracy: 0.7600\n",
      "Epoch 437/500\n",
      " - 0s - loss: 1.7755 - accuracy: 0.8400\n",
      "Epoch 438/500\n",
      " - 0s - loss: 1.7745 - accuracy: 0.7200\n",
      "Epoch 439/500\n",
      " - 0s - loss: 1.7742 - accuracy: 0.8000\n",
      "Epoch 440/500\n",
      " - 0s - loss: 1.7733 - accuracy: 0.7600\n",
      "Epoch 441/500\n",
      " - 0s - loss: 1.7724 - accuracy: 0.8000\n",
      "Epoch 442/500\n",
      " - 0s - loss: 1.7690 - accuracy: 0.7200\n",
      "Epoch 443/500\n",
      " - 0s - loss: 1.7673 - accuracy: 0.7600\n",
      "Epoch 444/500\n",
      " - 0s - loss: 1.7680 - accuracy: 0.7600\n",
      "Epoch 445/500\n",
      " - 0s - loss: 1.7663 - accuracy: 0.8000\n",
      "Epoch 446/500\n",
      " - 0s - loss: 1.7676 - accuracy: 0.7200\n",
      "Epoch 447/500\n",
      " - 0s - loss: 1.7663 - accuracy: 0.8000\n",
      "Epoch 448/500\n",
      " - 0s - loss: 1.7657 - accuracy: 0.7600\n",
      "Epoch 449/500\n",
      " - 0s - loss: 1.7623 - accuracy: 0.7200\n",
      "Epoch 450/500\n",
      " - 0s - loss: 1.7631 - accuracy: 0.8000\n",
      "Epoch 451/500\n",
      " - 0s - loss: 1.7623 - accuracy: 0.8000\n",
      "Epoch 452/500\n",
      " - 0s - loss: 1.7604 - accuracy: 0.7600\n",
      "Epoch 453/500\n",
      " - 0s - loss: 1.7586 - accuracy: 0.7600\n",
      "Epoch 454/500\n",
      " - 0s - loss: 1.7575 - accuracy: 0.7600\n",
      "Epoch 455/500\n",
      " - 0s - loss: 1.7580 - accuracy: 0.7200\n",
      "Epoch 456/500\n",
      " - 0s - loss: 1.7553 - accuracy: 0.7600\n",
      "Epoch 457/500\n",
      " - 0s - loss: 1.7552 - accuracy: 0.8000\n",
      "Epoch 458/500\n",
      " - 0s - loss: 1.7547 - accuracy: 0.7600\n",
      "Epoch 459/500\n",
      " - 0s - loss: 1.7534 - accuracy: 0.8400\n",
      "Epoch 460/500\n",
      " - 0s - loss: 1.7534 - accuracy: 0.7600\n",
      "Epoch 461/500\n",
      " - 0s - loss: 1.7519 - accuracy: 0.8000\n",
      "Epoch 462/500\n",
      " - 0s - loss: 1.7497 - accuracy: 0.8000\n",
      "Epoch 463/500\n",
      " - 0s - loss: 1.7502 - accuracy: 0.7200\n",
      "Epoch 464/500\n",
      " - 0s - loss: 1.7503 - accuracy: 0.7200\n",
      "Epoch 465/500\n",
      " - 0s - loss: 1.7483 - accuracy: 0.6800\n",
      "Epoch 466/500\n",
      " - 0s - loss: 1.7485 - accuracy: 0.7600\n",
      "Epoch 467/500\n",
      " - 0s - loss: 1.7455 - accuracy: 0.8000\n",
      "Epoch 468/500\n",
      " - 0s - loss: 1.7466 - accuracy: 0.8000\n",
      "Epoch 469/500\n",
      " - 0s - loss: 1.7438 - accuracy: 0.8000\n",
      "Epoch 470/500\n",
      " - 0s - loss: 1.7420 - accuracy: 0.8400\n",
      "Epoch 471/500\n",
      " - 0s - loss: 1.7426 - accuracy: 0.7600\n",
      "Epoch 472/500\n",
      " - 0s - loss: 1.7431 - accuracy: 0.8000\n",
      "Epoch 473/500\n",
      " - 0s - loss: 1.7409 - accuracy: 0.7600\n",
      "Epoch 474/500\n",
      " - 0s - loss: 1.7385 - accuracy: 0.7600\n",
      "Epoch 475/500\n",
      " - 0s - loss: 1.7408 - accuracy: 0.6800\n",
      "Epoch 476/500\n",
      " - 0s - loss: 1.7376 - accuracy: 0.8000\n",
      "Epoch 477/500\n",
      " - 0s - loss: 1.7390 - accuracy: 0.7200\n",
      "Epoch 478/500\n",
      " - 0s - loss: 1.7349 - accuracy: 0.7600\n",
      "Epoch 479/500\n",
      " - 0s - loss: 1.7328 - accuracy: 0.8000\n",
      "Epoch 480/500\n",
      " - 0s - loss: 1.7341 - accuracy: 0.7600\n",
      "Epoch 481/500\n",
      " - 0s - loss: 1.7333 - accuracy: 0.8000\n",
      "Epoch 482/500\n",
      " - 0s - loss: 1.7333 - accuracy: 0.7600\n",
      "Epoch 483/500\n",
      " - 0s - loss: 1.7304 - accuracy: 0.8000\n",
      "Epoch 484/500\n",
      " - 0s - loss: 1.7303 - accuracy: 0.7600\n",
      "Epoch 485/500\n",
      " - 0s - loss: 1.7303 - accuracy: 0.8000\n",
      "Epoch 486/500\n",
      " - 0s - loss: 1.7274 - accuracy: 0.8400\n",
      "Epoch 487/500\n",
      " - 0s - loss: 1.7287 - accuracy: 0.7600\n",
      "Epoch 488/500\n",
      " - 0s - loss: 1.7258 - accuracy: 0.8000\n",
      "Epoch 489/500\n",
      " - 0s - loss: 1.7264 - accuracy: 0.7600\n",
      "Epoch 490/500\n",
      " - 0s - loss: 1.7254 - accuracy: 0.8000\n",
      "Epoch 491/500\n",
      " - 0s - loss: 1.7233 - accuracy: 0.7200\n",
      "Epoch 492/500\n",
      " - 0s - loss: 1.7225 - accuracy: 0.7600\n",
      "Epoch 493/500\n",
      " - 0s - loss: 1.7234 - accuracy: 0.7600\n",
      "Epoch 494/500\n",
      " - 0s - loss: 1.7220 - accuracy: 0.7600\n",
      "Epoch 495/500\n",
      " - 0s - loss: 1.7194 - accuracy: 0.8400\n",
      "Epoch 496/500\n",
      " - 0s - loss: 1.7183 - accuracy: 0.8000\n",
      "Epoch 497/500\n",
      " - 0s - loss: 1.7185 - accuracy: 0.7600\n",
      "Epoch 498/500\n",
      " - 0s - loss: 1.7158 - accuracy: 0.7600\n",
      "Epoch 499/500\n",
      " - 0s - loss: 1.7158 - accuracy: 0.8000\n",
      "Epoch 500/500\n",
      " - 0s - loss: 1.7173 - accuracy: 0.7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd7e0ae12b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add( LSTM(32, input_shape= ( X.shape[1], X.shape[2] ) ) )\n",
    "model.add( Dense(y.shape[1] , activation= 'softmax' ) )\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "model.fit(X, y, epochs= 500, batch_size= 1, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy is : 80.00%\n"
     ]
    }
   ],
   "source": [
    "scores= model.evaluate(X, y, verbose= 0)\n",
    "print('Model Accuracy is : %.2f%%' %(scores[1]*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A'] -> B\n",
      "['B'] -> B\n",
      "['C'] -> D\n",
      "['D'] -> E\n",
      "['E'] -> F\n",
      "['F'] -> G\n",
      "['G'] -> H\n",
      "['H'] -> I\n",
      "['I'] -> J\n",
      "['J'] -> K\n",
      "['K'] -> L\n",
      "['L'] -> M\n",
      "['M'] -> N\n",
      "['N'] -> O\n",
      "['O'] -> P\n",
      "['P'] -> Q\n",
      "['Q'] -> R\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['T'] -> U\n",
      "['U'] -> W\n",
      "['V'] -> X\n",
      "['W'] -> Z\n",
      "['X'] -> Z\n",
      "['Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "for pattern in dataX:\n",
    "    x= np.reshape(pattern, (1, len(pattern), 1) )\n",
    "    x= x/float(len(alphabet))\n",
    "    prediction= model.predict_classes(x)[0]\n",
    "    #pdb.set_trace()\n",
    "    result= int_to_char[prediction]\n",
    "    seq_in= [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, '->', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the same problem with one-hot encoding of input data and an embedding layer\n",
    "\n",
    "### Before, we integer encoded X. Integer encoding is used when we want to use an embedding layer. If we use the integer encoded X as the input, the model assumes there is an ordinal relationship between characters, while there is none. They are not numbers.\n",
    "\n",
    "### For this reason, I will try to use an embedding layer and see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n"
     ]
    }
   ],
   "source": [
    "alphabet= \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# creat mapping of characters to integers and reverse\n",
    "char_to_int= dict ( [ (el, i) for i, el in enumerate(alphabet)] )\n",
    "int_to_char= dict( [ (i, el) for i, el in enumerate(alphabet)] )\n",
    "\n",
    "seq_length= 3\n",
    "dataX= []\n",
    "dataY= []\n",
    "for i in range( 0, len(alphabet) - seq_length, 1):\n",
    "    seq_in= alphabet[i: i+seq_length]\n",
    "    seq_out= alphabet[i+seq_length]\n",
    "    dataX.append( [char_to_int[char] for char in seq_in] )\n",
    "    dataY.append( char_to_int[seq_out] )\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (23, 3)\n",
      "y shape: (23, 26)\n"
     ]
    }
   ],
   "source": [
    "# change the format of the input X into [samples, timesteps]\n",
    "X= np.reshape( dataX, ( len(dataX), seq_length) )\n",
    "\n",
    "# one-hot encode the output as well\n",
    "y= np_utils.to_categorical(dataY)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 4)              108       \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32)                4736      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 5,702\n",
      "Trainable params: 5,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 3.2639 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.2552 - accuracy: 0.0435\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.2507 - accuracy: 0.0435\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.2459 - accuracy: 0.0435\n",
      "Epoch 5/100\n",
      " - 0s - loss: 3.2401 - accuracy: 0.0870\n",
      "Epoch 6/100\n",
      " - 0s - loss: 3.2329 - accuracy: 0.0870\n",
      "Epoch 7/100\n",
      " - 0s - loss: 3.2235 - accuracy: 0.0870\n",
      "Epoch 8/100\n",
      " - 0s - loss: 3.2102 - accuracy: 0.1304\n",
      "Epoch 9/100\n",
      " - 0s - loss: 3.1907 - accuracy: 0.0870\n",
      "Epoch 10/100\n",
      " - 0s - loss: 3.1601 - accuracy: 0.1739\n",
      "Epoch 11/100\n",
      " - 0s - loss: 3.1142 - accuracy: 0.2174\n",
      "Epoch 12/100\n",
      " - 0s - loss: 3.0417 - accuracy: 0.2174\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.9309 - accuracy: 0.2174\n",
      "Epoch 14/100\n",
      " - 0s - loss: 2.7995 - accuracy: 0.2174\n",
      "Epoch 15/100\n",
      " - 0s - loss: 2.6572 - accuracy: 0.2174\n",
      "Epoch 16/100\n",
      " - 0s - loss: 2.5307 - accuracy: 0.2174\n",
      "Epoch 17/100\n",
      " - 0s - loss: 2.4216 - accuracy: 0.1739\n",
      "Epoch 18/100\n",
      " - 0s - loss: 2.3247 - accuracy: 0.2174\n",
      "Epoch 19/100\n",
      " - 0s - loss: 2.2147 - accuracy: 0.2609\n",
      "Epoch 20/100\n",
      " - 0s - loss: 2.1012 - accuracy: 0.4783\n",
      "Epoch 21/100\n",
      " - 0s - loss: 1.9903 - accuracy: 0.4783\n",
      "Epoch 22/100\n",
      " - 0s - loss: 1.8686 - accuracy: 0.5217\n",
      "Epoch 23/100\n",
      " - 0s - loss: 1.7573 - accuracy: 0.5652\n",
      "Epoch 24/100\n",
      " - 0s - loss: 1.6505 - accuracy: 0.6087\n",
      "Epoch 25/100\n",
      " - 0s - loss: 1.5400 - accuracy: 0.6957\n",
      "Epoch 26/100\n",
      " - 0s - loss: 1.4452 - accuracy: 0.6522\n",
      "Epoch 27/100\n",
      " - 0s - loss: 1.3634 - accuracy: 0.7391\n",
      "Epoch 28/100\n",
      " - 0s - loss: 1.2834 - accuracy: 0.7826\n",
      "Epoch 29/100\n",
      " - 0s - loss: 1.2070 - accuracy: 0.8261\n",
      "Epoch 30/100\n",
      " - 0s - loss: 1.1452 - accuracy: 0.8696\n",
      "Epoch 31/100\n",
      " - 0s - loss: 1.0711 - accuracy: 0.8696\n",
      "Epoch 32/100\n",
      " - 0s - loss: 1.0119 - accuracy: 0.9130\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.9607 - accuracy: 0.8696\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.9056 - accuracy: 0.9565\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.8623 - accuracy: 0.8696\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.8184 - accuracy: 0.9565\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.7815 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.7438 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.7096 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.6740 - accuracy: 0.9565\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.6427 - accuracy: 0.9565\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.6118 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.5840 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.5624 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.5378 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5122 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4940 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4707 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4512 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4318 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4137 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.3950 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.3777 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.3641 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.3471 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.3361 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.3220 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.3078 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2953 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2836 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2743 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2630 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2522 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2422 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2343 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2248 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2182 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2096 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2021 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1956 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1886 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1819 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1762 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1709 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1658 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1603 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1556 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1501 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1454 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1416 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1375 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1331 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1295 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1259 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1219 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1187 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1152 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1120 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1093 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1060 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1038 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1006 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0983 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0955 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0933 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0909 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0887 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0864 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0845 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0823 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd7e0b9ad68>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size= len(alphabet) + 1 \n",
    "embedding_vector_length= 4\n",
    "length= seq_length\n",
    "\n",
    "model= Sequential()\n",
    "model.add( Embedding(vocab_size, embedding_vector_length, input_length= length) )\n",
    "model.add( LSTM(32) ) \n",
    "model.add( Dense(y.shape[1], activation= 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "model.fit(X, y, epochs= 100, batch_size= 1, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07985977828502655, 1.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> W\n",
      "['U', 'V', 'W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "for x in X:\n",
    "    #pdb.set_trace()\n",
    "    x= np.reshape( x, (1, seq_length))\n",
    "    predicted= model.predict_classes(x)\n",
    "    seq= [int_to_char[el] for el in x[0]]\n",
    "    output= int_to_char[predicted[0]]\n",
    "    print(seq, '->', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, the performance with embedding layer is perfect. Even with less number of epochs, we achieved an accuracy of almost 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering an elaborated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialString:\n",
    "    \"\"\"\n",
    "    This class creates a string with special indexing and slicing, in that out of bond\n",
    "    indices will return to the beginning of the sequence. For ex. if the length of the \n",
    "    sequence is 26, the index 25 is the last character and the index 26 normally gives an \n",
    "    error. However, in this special string, it goes back to the beginning of the string \n",
    "    and returns the first character again. The same logic with slicing\n",
    "    \"\"\"\n",
    "    def __init__(self, string_):\n",
    "        self.string= string_\n",
    "        self.index= 0\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        A= ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i' ]\n",
    "        A[7:8] -> ['h', 'i' ]\n",
    "        A[7:10] -> ['h', 'i', 'a', 'b']\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(key, slice):\n",
    "            start= key.start % len(self.string)\n",
    "            end= key.stop % len(self.string)\n",
    "            if end < start:\n",
    "                result= self.string[start:]\n",
    "                result += self.string[:end]\n",
    "                return result\n",
    "            else:\n",
    "                return self.string[start:end]\n",
    "        else:\n",
    "            index= key % len(self.string)\n",
    "            return self.string[index]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.string \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.string)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.index < len(self.string):\n",
    "            result= self.string[self.index]\n",
    "            self.index += 1\n",
    "            return result\n",
    "        else:\n",
    "            self.index= 0\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "Z -> A\n",
      "AB -> C\n",
      "BC -> D\n",
      "CD -> E\n",
      "DE -> F\n",
      "EF -> G\n",
      "FG -> H\n",
      "GH -> I\n",
      "HI -> J\n",
      "IJ -> K\n",
      "JK -> L\n",
      "KL -> M\n",
      "LM -> N\n",
      "MN -> O\n",
      "NO -> P\n",
      "OP -> Q\n",
      "PQ -> R\n",
      "QR -> S\n",
      "RS -> T\n",
      "ST -> U\n",
      "TU -> V\n",
      "UV -> W\n",
      "VW -> X\n",
      "WX -> Y\n",
      "XY -> Z\n",
      "YZ -> A\n",
      "ZA -> B\n",
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n",
      "XYZ -> A\n",
      "YZA -> B\n",
      "ZAB -> C\n",
      "ABCD -> E\n",
      "BCDE -> F\n",
      "CDEF -> G\n",
      "DEFG -> H\n",
      "EFGH -> I\n",
      "FGHI -> J\n",
      "GHIJ -> K\n",
      "HIJK -> L\n",
      "IJKL -> M\n",
      "JKLM -> N\n",
      "KLMN -> O\n",
      "LMNO -> P\n",
      "MNOP -> Q\n",
      "NOPQ -> R\n",
      "OPQR -> S\n",
      "PQRS -> T\n",
      "QRST -> U\n",
      "RSTU -> V\n",
      "STUV -> W\n",
      "TUVW -> X\n",
      "UVWX -> Y\n",
      "VWXY -> Z\n",
      "WXYZ -> A\n",
      "XYZA -> B\n",
      "YZAB -> C\n",
      "ZABC -> D\n",
      "ABCDE -> F\n",
      "BCDEF -> G\n",
      "CDEFG -> H\n",
      "DEFGH -> I\n",
      "EFGHI -> J\n",
      "FGHIJ -> K\n",
      "GHIJK -> L\n",
      "HIJKL -> M\n",
      "IJKLM -> N\n",
      "JKLMN -> O\n",
      "KLMNO -> P\n",
      "LMNOP -> Q\n",
      "MNOPQ -> R\n",
      "NOPQR -> S\n",
      "OPQRS -> T\n",
      "PQRST -> U\n",
      "QRSTU -> V\n",
      "RSTUV -> W\n",
      "STUVW -> X\n",
      "TUVWX -> Y\n",
      "UVWXY -> Z\n",
      "VWXYZ -> A\n",
      "WXYZA -> B\n",
      "XYZAB -> C\n",
      "YZABC -> D\n",
      "ZABCD -> E\n",
      "ABCDEF -> G\n",
      "BCDEFG -> H\n",
      "CDEFGH -> I\n",
      "DEFGHI -> J\n",
      "EFGHIJ -> K\n",
      "FGHIJK -> L\n",
      "GHIJKL -> M\n",
      "HIJKLM -> N\n",
      "IJKLMN -> O\n",
      "JKLMNO -> P\n",
      "KLMNOP -> Q\n",
      "LMNOPQ -> R\n",
      "MNOPQR -> S\n",
      "NOPQRS -> T\n",
      "OPQRST -> U\n",
      "PQRSTU -> V\n",
      "QRSTUV -> W\n",
      "RSTUVW -> X\n",
      "STUVWX -> Y\n",
      "TUVWXY -> Z\n",
      "UVWXYZ -> A\n",
      "VWXYZA -> B\n",
      "WXYZAB -> C\n",
      "XYZABC -> D\n",
      "YZABCD -> E\n",
      "ZABCDE -> F\n",
      "ABCDEFG -> H\n",
      "BCDEFGH -> I\n",
      "CDEFGHI -> J\n",
      "DEFGHIJ -> K\n",
      "EFGHIJK -> L\n",
      "FGHIJKL -> M\n",
      "GHIJKLM -> N\n",
      "HIJKLMN -> O\n",
      "IJKLMNO -> P\n",
      "JKLMNOP -> Q\n",
      "KLMNOPQ -> R\n",
      "LMNOPQR -> S\n",
      "MNOPQRS -> T\n",
      "NOPQRST -> U\n",
      "OPQRSTU -> V\n",
      "PQRSTUV -> W\n",
      "QRSTUVW -> X\n",
      "RSTUVWX -> Y\n",
      "STUVWXY -> Z\n",
      "TUVWXYZ -> A\n",
      "UVWXYZA -> B\n",
      "VWXYZAB -> C\n",
      "WXYZABC -> D\n",
      "XYZABCD -> E\n",
      "YZABCDE -> F\n",
      "ZABCDEF -> G\n",
      "ABCDEFGH -> I\n",
      "BCDEFGHI -> J\n",
      "CDEFGHIJ -> K\n",
      "DEFGHIJK -> L\n",
      "EFGHIJKL -> M\n",
      "FGHIJKLM -> N\n",
      "GHIJKLMN -> O\n",
      "HIJKLMNO -> P\n",
      "IJKLMNOP -> Q\n",
      "JKLMNOPQ -> R\n",
      "KLMNOPQR -> S\n",
      "LMNOPQRS -> T\n",
      "MNOPQRST -> U\n",
      "NOPQRSTU -> V\n",
      "OPQRSTUV -> W\n",
      "PQRSTUVW -> X\n",
      "QRSTUVWX -> Y\n",
      "RSTUVWXY -> Z\n",
      "STUVWXYZ -> A\n",
      "TUVWXYZA -> B\n",
      "UVWXYZAB -> C\n",
      "VWXYZABC -> D\n",
      "WXYZABCD -> E\n",
      "XYZABCDE -> F\n",
      "YZABCDEF -> G\n",
      "ZABCDEFG -> H\n",
      "ABCDEFGHI -> J\n",
      "BCDEFGHIJ -> K\n",
      "CDEFGHIJK -> L\n",
      "DEFGHIJKL -> M\n",
      "EFGHIJKLM -> N\n",
      "FGHIJKLMN -> O\n",
      "GHIJKLMNO -> P\n",
      "HIJKLMNOP -> Q\n",
      "IJKLMNOPQ -> R\n",
      "JKLMNOPQR -> S\n",
      "KLMNOPQRS -> T\n",
      "LMNOPQRST -> U\n",
      "MNOPQRSTU -> V\n",
      "NOPQRSTUV -> W\n",
      "OPQRSTUVW -> X\n",
      "PQRSTUVWX -> Y\n",
      "QRSTUVWXY -> Z\n",
      "RSTUVWXYZ -> A\n",
      "STUVWXYZA -> B\n",
      "TUVWXYZAB -> C\n",
      "UVWXYZABC -> D\n",
      "VWXYZABCD -> E\n",
      "WXYZABCDE -> F\n",
      "XYZABCDEF -> G\n",
      "YZABCDEFG -> H\n",
      "ZABCDEFGH -> I\n",
      "ABCDEFGHIJ -> K\n",
      "BCDEFGHIJK -> L\n",
      "CDEFGHIJKL -> M\n",
      "DEFGHIJKLM -> N\n",
      "EFGHIJKLMN -> O\n",
      "FGHIJKLMNO -> P\n",
      "GHIJKLMNOP -> Q\n",
      "HIJKLMNOPQ -> R\n",
      "IJKLMNOPQR -> S\n",
      "JKLMNOPQRS -> T\n",
      "KLMNOPQRST -> U\n",
      "LMNOPQRSTU -> V\n",
      "MNOPQRSTUV -> W\n",
      "NOPQRSTUVW -> X\n",
      "OPQRSTUVWX -> Y\n",
      "PQRSTUVWXY -> Z\n",
      "QRSTUVWXYZ -> A\n",
      "RSTUVWXYZA -> B\n",
      "STUVWXYZAB -> C\n",
      "TUVWXYZABC -> D\n",
      "UVWXYZABCD -> E\n",
      "VWXYZABCDE -> F\n",
      "WXYZABCDEF -> G\n",
      "XYZABCDEFG -> H\n",
      "YZABCDEFGH -> I\n",
      "ZABCDEFGHI -> J\n",
      "ABCDEFGHIJK -> L\n",
      "BCDEFGHIJKL -> M\n",
      "CDEFGHIJKLM -> N\n",
      "DEFGHIJKLMN -> O\n",
      "EFGHIJKLMNO -> P\n",
      "FGHIJKLMNOP -> Q\n",
      "GHIJKLMNOPQ -> R\n",
      "HIJKLMNOPQR -> S\n",
      "IJKLMNOPQRS -> T\n",
      "JKLMNOPQRST -> U\n",
      "KLMNOPQRSTU -> V\n",
      "LMNOPQRSTUV -> W\n",
      "MNOPQRSTUVW -> X\n",
      "NOPQRSTUVWX -> Y\n",
      "OPQRSTUVWXY -> Z\n",
      "PQRSTUVWXYZ -> A\n",
      "QRSTUVWXYZA -> B\n",
      "RSTUVWXYZAB -> C\n",
      "STUVWXYZABC -> D\n",
      "TUVWXYZABCD -> E\n",
      "UVWXYZABCDE -> F\n",
      "VWXYZABCDEF -> G\n",
      "WXYZABCDEFG -> H\n",
      "XYZABCDEFGH -> I\n",
      "YZABCDEFGHI -> J\n",
      "ZABCDEFGHIJ -> K\n",
      "ABCDEFGHIJKL -> M\n",
      "BCDEFGHIJKLM -> N\n",
      "CDEFGHIJKLMN -> O\n",
      "DEFGHIJKLMNO -> P\n",
      "EFGHIJKLMNOP -> Q\n",
      "FGHIJKLMNOPQ -> R\n",
      "GHIJKLMNOPQR -> S\n",
      "HIJKLMNOPQRS -> T\n",
      "IJKLMNOPQRST -> U\n",
      "JKLMNOPQRSTU -> V\n",
      "KLMNOPQRSTUV -> W\n",
      "LMNOPQRSTUVW -> X\n",
      "MNOPQRSTUVWX -> Y\n",
      "NOPQRSTUVWXY -> Z\n",
      "OPQRSTUVWXYZ -> A\n",
      "PQRSTUVWXYZA -> B\n",
      "QRSTUVWXYZAB -> C\n",
      "RSTUVWXYZABC -> D\n",
      "STUVWXYZABCD -> E\n",
      "TUVWXYZABCDE -> F\n",
      "UVWXYZABCDEF -> G\n",
      "VWXYZABCDEFG -> H\n",
      "WXYZABCDEFGH -> I\n",
      "XYZABCDEFGHI -> J\n",
      "YZABCDEFGHIJ -> K\n",
      "ZABCDEFGHIJK -> L\n",
      "ABCDEFGHIJKLM -> N\n",
      "BCDEFGHIJKLMN -> O\n",
      "CDEFGHIJKLMNO -> P\n",
      "DEFGHIJKLMNOP -> Q\n",
      "EFGHIJKLMNOPQ -> R\n",
      "FGHIJKLMNOPQR -> S\n",
      "GHIJKLMNOPQRS -> T\n",
      "HIJKLMNOPQRST -> U\n",
      "IJKLMNOPQRSTU -> V\n",
      "JKLMNOPQRSTUV -> W\n",
      "KLMNOPQRSTUVW -> X\n",
      "LMNOPQRSTUVWX -> Y\n",
      "MNOPQRSTUVWXY -> Z\n",
      "NOPQRSTUVWXYZ -> A\n",
      "OPQRSTUVWXYZA -> B\n",
      "PQRSTUVWXYZAB -> C\n",
      "QRSTUVWXYZABC -> D\n",
      "RSTUVWXYZABCD -> E\n",
      "STUVWXYZABCDE -> F\n",
      "TUVWXYZABCDEF -> G\n",
      "UVWXYZABCDEFG -> H\n",
      "VWXYZABCDEFGH -> I\n",
      "WXYZABCDEFGHI -> J\n",
      "XYZABCDEFGHIJ -> K\n",
      "YZABCDEFGHIJK -> L\n",
      "ZABCDEFGHIJKL -> M\n",
      "ABCDEFGHIJKLMN -> O\n",
      "BCDEFGHIJKLMNO -> P\n",
      "CDEFGHIJKLMNOP -> Q\n",
      "DEFGHIJKLMNOPQ -> R\n",
      "EFGHIJKLMNOPQR -> S\n",
      "FGHIJKLMNOPQRS -> T\n",
      "GHIJKLMNOPQRST -> U\n",
      "HIJKLMNOPQRSTU -> V\n",
      "IJKLMNOPQRSTUV -> W\n",
      "JKLMNOPQRSTUVW -> X\n",
      "KLMNOPQRSTUVWX -> Y\n",
      "LMNOPQRSTUVWXY -> Z\n",
      "MNOPQRSTUVWXYZ -> A\n",
      "NOPQRSTUVWXYZA -> B\n",
      "OPQRSTUVWXYZAB -> C\n",
      "PQRSTUVWXYZABC -> D\n",
      "QRSTUVWXYZABCD -> E\n",
      "RSTUVWXYZABCDE -> F\n",
      "STUVWXYZABCDEF -> G\n",
      "TUVWXYZABCDEFG -> H\n",
      "UVWXYZABCDEFGH -> I\n",
      "VWXYZABCDEFGHI -> J\n",
      "WXYZABCDEFGHIJ -> K\n",
      "XYZABCDEFGHIJK -> L\n",
      "YZABCDEFGHIJKL -> M\n",
      "ZABCDEFGHIJKLM -> N\n",
      "ABCDEFGHIJKLMNO -> P\n",
      "BCDEFGHIJKLMNOP -> Q\n",
      "CDEFGHIJKLMNOPQ -> R\n",
      "DEFGHIJKLMNOPQR -> S\n",
      "EFGHIJKLMNOPQRS -> T\n",
      "FGHIJKLMNOPQRST -> U\n",
      "GHIJKLMNOPQRSTU -> V\n",
      "HIJKLMNOPQRSTUV -> W\n",
      "IJKLMNOPQRSTUVW -> X\n",
      "JKLMNOPQRSTUVWX -> Y\n",
      "KLMNOPQRSTUVWXY -> Z\n",
      "LMNOPQRSTUVWXYZ -> A\n",
      "MNOPQRSTUVWXYZA -> B\n",
      "NOPQRSTUVWXYZAB -> C\n",
      "OPQRSTUVWXYZABC -> D\n",
      "PQRSTUVWXYZABCD -> E\n",
      "QRSTUVWXYZABCDE -> F\n",
      "RSTUVWXYZABCDEF -> G\n",
      "STUVWXYZABCDEFG -> H\n",
      "TUVWXYZABCDEFGH -> I\n",
      "UVWXYZABCDEFGHI -> J\n",
      "VWXYZABCDEFGHIJ -> K\n",
      "WXYZABCDEFGHIJK -> L\n",
      "XYZABCDEFGHIJKL -> M\n",
      "YZABCDEFGHIJKLM -> N\n",
      "ZABCDEFGHIJKLMN -> O\n",
      "ABCDEFGHIJKLMNOP -> Q\n",
      "BCDEFGHIJKLMNOPQ -> R\n",
      "CDEFGHIJKLMNOPQR -> S\n",
      "DEFGHIJKLMNOPQRS -> T\n",
      "EFGHIJKLMNOPQRST -> U\n",
      "FGHIJKLMNOPQRSTU -> V\n",
      "GHIJKLMNOPQRSTUV -> W\n",
      "HIJKLMNOPQRSTUVW -> X\n",
      "IJKLMNOPQRSTUVWX -> Y\n",
      "JKLMNOPQRSTUVWXY -> Z\n",
      "KLMNOPQRSTUVWXYZ -> A\n",
      "LMNOPQRSTUVWXYZA -> B\n",
      "MNOPQRSTUVWXYZAB -> C\n",
      "NOPQRSTUVWXYZABC -> D\n",
      "OPQRSTUVWXYZABCD -> E\n",
      "PQRSTUVWXYZABCDE -> F\n",
      "QRSTUVWXYZABCDEF -> G\n",
      "RSTUVWXYZABCDEFG -> H\n",
      "STUVWXYZABCDEFGH -> I\n",
      "TUVWXYZABCDEFGHI -> J\n",
      "UVWXYZABCDEFGHIJ -> K\n",
      "VWXYZABCDEFGHIJK -> L\n",
      "WXYZABCDEFGHIJKL -> M\n",
      "XYZABCDEFGHIJKLM -> N\n",
      "YZABCDEFGHIJKLMN -> O\n",
      "ZABCDEFGHIJKLMNO -> P\n",
      "ABCDEFGHIJKLMNOPQ -> R\n",
      "BCDEFGHIJKLMNOPQR -> S\n",
      "CDEFGHIJKLMNOPQRS -> T\n",
      "DEFGHIJKLMNOPQRST -> U\n",
      "EFGHIJKLMNOPQRSTU -> V\n",
      "FGHIJKLMNOPQRSTUV -> W\n",
      "GHIJKLMNOPQRSTUVW -> X\n",
      "HIJKLMNOPQRSTUVWX -> Y\n",
      "IJKLMNOPQRSTUVWXY -> Z\n",
      "JKLMNOPQRSTUVWXYZ -> A\n",
      "KLMNOPQRSTUVWXYZA -> B\n",
      "LMNOPQRSTUVWXYZAB -> C\n",
      "MNOPQRSTUVWXYZABC -> D\n",
      "NOPQRSTUVWXYZABCD -> E\n",
      "OPQRSTUVWXYZABCDE -> F\n",
      "PQRSTUVWXYZABCDEF -> G\n",
      "QRSTUVWXYZABCDEFG -> H\n",
      "RSTUVWXYZABCDEFGH -> I\n",
      "STUVWXYZABCDEFGHI -> J\n",
      "TUVWXYZABCDEFGHIJ -> K\n",
      "UVWXYZABCDEFGHIJK -> L\n",
      "VWXYZABCDEFGHIJKL -> M\n",
      "WXYZABCDEFGHIJKLM -> N\n",
      "XYZABCDEFGHIJKLMN -> O\n",
      "YZABCDEFGHIJKLMNO -> P\n",
      "ZABCDEFGHIJKLMNOP -> Q\n",
      "ABCDEFGHIJKLMNOPQR -> S\n",
      "BCDEFGHIJKLMNOPQRS -> T\n",
      "CDEFGHIJKLMNOPQRST -> U\n",
      "DEFGHIJKLMNOPQRSTU -> V\n",
      "EFGHIJKLMNOPQRSTUV -> W\n",
      "FGHIJKLMNOPQRSTUVW -> X\n",
      "GHIJKLMNOPQRSTUVWX -> Y\n",
      "HIJKLMNOPQRSTUVWXY -> Z\n",
      "IJKLMNOPQRSTUVWXYZ -> A\n",
      "JKLMNOPQRSTUVWXYZA -> B\n",
      "KLMNOPQRSTUVWXYZAB -> C\n",
      "LMNOPQRSTUVWXYZABC -> D\n",
      "MNOPQRSTUVWXYZABCD -> E\n",
      "NOPQRSTUVWXYZABCDE -> F\n",
      "OPQRSTUVWXYZABCDEF -> G\n",
      "PQRSTUVWXYZABCDEFG -> H\n",
      "QRSTUVWXYZABCDEFGH -> I\n",
      "RSTUVWXYZABCDEFGHI -> J\n",
      "STUVWXYZABCDEFGHIJ -> K\n",
      "TUVWXYZABCDEFGHIJK -> L\n",
      "UVWXYZABCDEFGHIJKL -> M\n",
      "VWXYZABCDEFGHIJKLM -> N\n",
      "WXYZABCDEFGHIJKLMN -> O\n",
      "XYZABCDEFGHIJKLMNO -> P\n",
      "YZABCDEFGHIJKLMNOP -> Q\n",
      "ZABCDEFGHIJKLMNOPQ -> R\n",
      "ABCDEFGHIJKLMNOPQRS -> T\n",
      "BCDEFGHIJKLMNOPQRST -> U\n",
      "CDEFGHIJKLMNOPQRSTU -> V\n",
      "DEFGHIJKLMNOPQRSTUV -> W\n",
      "EFGHIJKLMNOPQRSTUVW -> X\n",
      "FGHIJKLMNOPQRSTUVWX -> Y\n",
      "GHIJKLMNOPQRSTUVWXY -> Z\n",
      "HIJKLMNOPQRSTUVWXYZ -> A\n",
      "IJKLMNOPQRSTUVWXYZA -> B\n",
      "JKLMNOPQRSTUVWXYZAB -> C\n",
      "KLMNOPQRSTUVWXYZABC -> D\n",
      "LMNOPQRSTUVWXYZABCD -> E\n",
      "MNOPQRSTUVWXYZABCDE -> F\n",
      "NOPQRSTUVWXYZABCDEF -> G\n",
      "OPQRSTUVWXYZABCDEFG -> H\n",
      "PQRSTUVWXYZABCDEFGH -> I\n",
      "QRSTUVWXYZABCDEFGHI -> J\n",
      "RSTUVWXYZABCDEFGHIJ -> K\n",
      "STUVWXYZABCDEFGHIJK -> L\n",
      "TUVWXYZABCDEFGHIJKL -> M\n",
      "UVWXYZABCDEFGHIJKLM -> N\n",
      "VWXYZABCDEFGHIJKLMN -> O\n",
      "WXYZABCDEFGHIJKLMNO -> P\n",
      "XYZABCDEFGHIJKLMNOP -> Q\n",
      "YZABCDEFGHIJKLMNOPQ -> R\n",
      "ZABCDEFGHIJKLMNOPQR -> S\n",
      "ABCDEFGHIJKLMNOPQRST -> U\n",
      "BCDEFGHIJKLMNOPQRSTU -> V\n",
      "CDEFGHIJKLMNOPQRSTUV -> W\n",
      "DEFGHIJKLMNOPQRSTUVW -> X\n",
      "EFGHIJKLMNOPQRSTUVWX -> Y\n",
      "FGHIJKLMNOPQRSTUVWXY -> Z\n",
      "GHIJKLMNOPQRSTUVWXYZ -> A\n",
      "HIJKLMNOPQRSTUVWXYZA -> B\n",
      "IJKLMNOPQRSTUVWXYZAB -> C\n",
      "JKLMNOPQRSTUVWXYZABC -> D\n",
      "KLMNOPQRSTUVWXYZABCD -> E\n",
      "LMNOPQRSTUVWXYZABCDE -> F\n",
      "MNOPQRSTUVWXYZABCDEF -> G\n",
      "NOPQRSTUVWXYZABCDEFG -> H\n",
      "OPQRSTUVWXYZABCDEFGH -> I\n",
      "PQRSTUVWXYZABCDEFGHI -> J\n",
      "QRSTUVWXYZABCDEFGHIJ -> K\n",
      "RSTUVWXYZABCDEFGHIJK -> L\n",
      "STUVWXYZABCDEFGHIJKL -> M\n",
      "TUVWXYZABCDEFGHIJKLM -> N\n",
      "UVWXYZABCDEFGHIJKLMN -> O\n",
      "VWXYZABCDEFGHIJKLMNO -> P\n",
      "WXYZABCDEFGHIJKLMNOP -> Q\n",
      "XYZABCDEFGHIJKLMNOPQ -> R\n",
      "YZABCDEFGHIJKLMNOPQR -> S\n",
      "ZABCDEFGHIJKLMNOPQRS -> T\n",
      "ABCDEFGHIJKLMNOPQRSTU -> V\n",
      "BCDEFGHIJKLMNOPQRSTUV -> W\n",
      "CDEFGHIJKLMNOPQRSTUVW -> X\n",
      "DEFGHIJKLMNOPQRSTUVWX -> Y\n",
      "EFGHIJKLMNOPQRSTUVWXY -> Z\n",
      "FGHIJKLMNOPQRSTUVWXYZ -> A\n",
      "GHIJKLMNOPQRSTUVWXYZA -> B\n",
      "HIJKLMNOPQRSTUVWXYZAB -> C\n",
      "IJKLMNOPQRSTUVWXYZABC -> D\n",
      "JKLMNOPQRSTUVWXYZABCD -> E\n",
      "KLMNOPQRSTUVWXYZABCDE -> F\n",
      "LMNOPQRSTUVWXYZABCDEF -> G\n",
      "MNOPQRSTUVWXYZABCDEFG -> H\n",
      "NOPQRSTUVWXYZABCDEFGH -> I\n",
      "OPQRSTUVWXYZABCDEFGHI -> J\n",
      "PQRSTUVWXYZABCDEFGHIJ -> K\n",
      "QRSTUVWXYZABCDEFGHIJK -> L\n",
      "RSTUVWXYZABCDEFGHIJKL -> M\n",
      "STUVWXYZABCDEFGHIJKLM -> N\n",
      "TUVWXYZABCDEFGHIJKLMN -> O\n",
      "UVWXYZABCDEFGHIJKLMNO -> P\n",
      "VWXYZABCDEFGHIJKLMNOP -> Q\n",
      "WXYZABCDEFGHIJKLMNOPQ -> R\n",
      "XYZABCDEFGHIJKLMNOPQR -> S\n",
      "YZABCDEFGHIJKLMNOPQRS -> T\n",
      "ZABCDEFGHIJKLMNOPQRST -> U\n",
      "ABCDEFGHIJKLMNOPQRSTUV -> W\n",
      "BCDEFGHIJKLMNOPQRSTUVW -> X\n",
      "CDEFGHIJKLMNOPQRSTUVWX -> Y\n",
      "DEFGHIJKLMNOPQRSTUVWXY -> Z\n",
      "EFGHIJKLMNOPQRSTUVWXYZ -> A\n",
      "FGHIJKLMNOPQRSTUVWXYZA -> B\n",
      "GHIJKLMNOPQRSTUVWXYZAB -> C\n",
      "HIJKLMNOPQRSTUVWXYZABC -> D\n",
      "IJKLMNOPQRSTUVWXYZABCD -> E\n",
      "JKLMNOPQRSTUVWXYZABCDE -> F\n",
      "KLMNOPQRSTUVWXYZABCDEF -> G\n",
      "LMNOPQRSTUVWXYZABCDEFG -> H\n",
      "MNOPQRSTUVWXYZABCDEFGH -> I\n",
      "NOPQRSTUVWXYZABCDEFGHI -> J\n",
      "OPQRSTUVWXYZABCDEFGHIJ -> K\n",
      "PQRSTUVWXYZABCDEFGHIJK -> L\n",
      "QRSTUVWXYZABCDEFGHIJKL -> M\n",
      "RSTUVWXYZABCDEFGHIJKLM -> N\n",
      "STUVWXYZABCDEFGHIJKLMN -> O\n",
      "TUVWXYZABCDEFGHIJKLMNO -> P\n",
      "UVWXYZABCDEFGHIJKLMNOP -> Q\n",
      "VWXYZABCDEFGHIJKLMNOPQ -> R\n",
      "WXYZABCDEFGHIJKLMNOPQR -> S\n",
      "XYZABCDEFGHIJKLMNOPQRS -> T\n",
      "YZABCDEFGHIJKLMNOPQRST -> U\n",
      "ZABCDEFGHIJKLMNOPQRSTU -> V\n",
      "ABCDEFGHIJKLMNOPQRSTUVW -> X\n",
      "BCDEFGHIJKLMNOPQRSTUVWX -> Y\n",
      "CDEFGHIJKLMNOPQRSTUVWXY -> Z\n",
      "DEFGHIJKLMNOPQRSTUVWXYZ -> A\n",
      "EFGHIJKLMNOPQRSTUVWXYZA -> B\n",
      "FGHIJKLMNOPQRSTUVWXYZAB -> C\n",
      "GHIJKLMNOPQRSTUVWXYZABC -> D\n",
      "HIJKLMNOPQRSTUVWXYZABCD -> E\n",
      "IJKLMNOPQRSTUVWXYZABCDE -> F\n",
      "JKLMNOPQRSTUVWXYZABCDEF -> G\n",
      "KLMNOPQRSTUVWXYZABCDEFG -> H\n",
      "LMNOPQRSTUVWXYZABCDEFGH -> I\n",
      "MNOPQRSTUVWXYZABCDEFGHI -> J\n",
      "NOPQRSTUVWXYZABCDEFGHIJ -> K\n",
      "OPQRSTUVWXYZABCDEFGHIJK -> L\n",
      "PQRSTUVWXYZABCDEFGHIJKL -> M\n",
      "QRSTUVWXYZABCDEFGHIJKLM -> N\n",
      "RSTUVWXYZABCDEFGHIJKLMN -> O\n",
      "STUVWXYZABCDEFGHIJKLMNO -> P\n",
      "TUVWXYZABCDEFGHIJKLMNOP -> Q\n",
      "UVWXYZABCDEFGHIJKLMNOPQ -> R\n",
      "VWXYZABCDEFGHIJKLMNOPQR -> S\n",
      "WXYZABCDEFGHIJKLMNOPQRS -> T\n",
      "XYZABCDEFGHIJKLMNOPQRST -> U\n",
      "YZABCDEFGHIJKLMNOPQRSTU -> V\n",
      "ZABCDEFGHIJKLMNOPQRSTUV -> W\n",
      "ABCDEFGHIJKLMNOPQRSTUVWX -> Y\n",
      "BCDEFGHIJKLMNOPQRSTUVWXY -> Z\n",
      "CDEFGHIJKLMNOPQRSTUVWXYZ -> A\n",
      "DEFGHIJKLMNOPQRSTUVWXYZA -> B\n",
      "EFGHIJKLMNOPQRSTUVWXYZAB -> C\n",
      "FGHIJKLMNOPQRSTUVWXYZABC -> D\n",
      "GHIJKLMNOPQRSTUVWXYZABCD -> E\n",
      "HIJKLMNOPQRSTUVWXYZABCDE -> F\n",
      "IJKLMNOPQRSTUVWXYZABCDEF -> G\n",
      "JKLMNOPQRSTUVWXYZABCDEFG -> H\n",
      "KLMNOPQRSTUVWXYZABCDEFGH -> I\n",
      "LMNOPQRSTUVWXYZABCDEFGHI -> J\n",
      "MNOPQRSTUVWXYZABCDEFGHIJ -> K\n",
      "NOPQRSTUVWXYZABCDEFGHIJK -> L\n",
      "OPQRSTUVWXYZABCDEFGHIJKL -> M\n",
      "PQRSTUVWXYZABCDEFGHIJKLM -> N\n",
      "QRSTUVWXYZABCDEFGHIJKLMN -> O\n",
      "RSTUVWXYZABCDEFGHIJKLMNO -> P\n",
      "STUVWXYZABCDEFGHIJKLMNOP -> Q\n",
      "TUVWXYZABCDEFGHIJKLMNOPQ -> R\n",
      "UVWXYZABCDEFGHIJKLMNOPQR -> S\n",
      "VWXYZABCDEFGHIJKLMNOPQRS -> T\n",
      "WXYZABCDEFGHIJKLMNOPQRST -> U\n",
      "XYZABCDEFGHIJKLMNOPQRSTU -> V\n",
      "YZABCDEFGHIJKLMNOPQRSTUV -> W\n",
      "ZABCDEFGHIJKLMNOPQRSTUVW -> X\n",
      "ABCDEFGHIJKLMNOPQRSTUVWXY -> Z\n",
      "BCDEFGHIJKLMNOPQRSTUVWXYZ -> A\n",
      "CDEFGHIJKLMNOPQRSTUVWXYZA -> B\n",
      "DEFGHIJKLMNOPQRSTUVWXYZAB -> C\n",
      "EFGHIJKLMNOPQRSTUVWXYZABC -> D\n",
      "FGHIJKLMNOPQRSTUVWXYZABCD -> E\n",
      "GHIJKLMNOPQRSTUVWXYZABCDE -> F\n",
      "HIJKLMNOPQRSTUVWXYZABCDEF -> G\n",
      "IJKLMNOPQRSTUVWXYZABCDEFG -> H\n",
      "JKLMNOPQRSTUVWXYZABCDEFGH -> I\n",
      "KLMNOPQRSTUVWXYZABCDEFGHI -> J\n",
      "LMNOPQRSTUVWXYZABCDEFGHIJ -> K\n",
      "MNOPQRSTUVWXYZABCDEFGHIJK -> L\n",
      "NOPQRSTUVWXYZABCDEFGHIJKL -> M\n",
      "OPQRSTUVWXYZABCDEFGHIJKLM -> N\n",
      "PQRSTUVWXYZABCDEFGHIJKLMN -> O\n",
      "QRSTUVWXYZABCDEFGHIJKLMNO -> P\n",
      "RSTUVWXYZABCDEFGHIJKLMNOP -> Q\n",
      "STUVWXYZABCDEFGHIJKLMNOPQ -> R\n",
      "TUVWXYZABCDEFGHIJKLMNOPQR -> S\n",
      "UVWXYZABCDEFGHIJKLMNOPQRS -> T\n",
      "VWXYZABCDEFGHIJKLMNOPQRST -> U\n",
      "WXYZABCDEFGHIJKLMNOPQRSTU -> V\n",
      "XYZABCDEFGHIJKLMNOPQRSTUV -> W\n",
      "YZABCDEFGHIJKLMNOPQRSTUVW -> X\n",
      "ZABCDEFGHIJKLMNOPQRSTUVWX -> Y\n"
     ]
    }
   ],
   "source": [
    "alphabet= SpecialString( \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" )\n",
    "# creat mapping of characters to integers and reverse\n",
    "char_to_int= dict ( [ (el, i) for i, el in enumerate(alphabet)] )\n",
    "int_to_char= dict( [ (i, el) for i, el in enumerate(alphabet)] )\n",
    "\n",
    "\n",
    "dataX= []\n",
    "dataY= []\n",
    "for seq_length in range(1, 26):\n",
    "    for i in range( 0, len(alphabet), 1):\n",
    "        seq_in= alphabet[i: i+seq_length]\n",
    "        #pdb.set_trace()\n",
    "        seq_out= alphabet[i+seq_length]\n",
    "        dataX.append( [char_to_int[char] for char in seq_in] )\n",
    "        dataY.append( char_to_int[seq_out] )\n",
    "        print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the sequences\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (650, 25)\n",
      "y shape: (650, 26)\n"
     ]
    }
   ],
   "source": [
    "# for padding we do not use 0 as it has already been reserved for \"A\".\n",
    "# So we instead use 26 = len(alphabet) \n",
    "# However, let's add it to out dictionaries\n",
    "int_to_char[26]= ''\n",
    "\n",
    "seq_length= len(alphabet) - 1\n",
    "dataX_padded= sequence.pad_sequences(dataX, maxlen= seq_length, value= len(alphabet) )\n",
    "\n",
    "# change the format of the input X into [samples, timesteps]\n",
    "X= np.reshape( dataX_padded, ( len(dataX_padded), seq_length) )\n",
    "\n",
    "# one-hot encode the output as well\n",
    "y= np_utils.to_categorical(dataY)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 4)             108       \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 32)                4736      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 5,702\n",
      "Trainable params: 5,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 2s - loss: 3.2528 - accuracy: 0.0754\n",
      "Epoch 2/100\n",
      " - 2s - loss: 3.0283 - accuracy: 0.1354\n",
      "Epoch 3/100\n",
      " - 2s - loss: 2.5554 - accuracy: 0.1569\n",
      "Epoch 4/100\n",
      " - 2s - loss: 2.2808 - accuracy: 0.2400\n",
      "Epoch 5/100\n",
      " - 2s - loss: 2.1129 - accuracy: 0.3446\n",
      "Epoch 6/100\n",
      " - 2s - loss: 1.9658 - accuracy: 0.3862\n",
      "Epoch 7/100\n",
      " - 2s - loss: 1.8553 - accuracy: 0.4262\n",
      "Epoch 8/100\n",
      " - 2s - loss: 1.7257 - accuracy: 0.5292\n",
      "Epoch 9/100\n",
      " - 2s - loss: 1.6098 - accuracy: 0.5754\n",
      "Epoch 10/100\n",
      " - 2s - loss: 1.5379 - accuracy: 0.6677\n",
      "Epoch 11/100\n",
      " - 2s - loss: 1.4326 - accuracy: 0.6846\n",
      "Epoch 12/100\n",
      " - 2s - loss: 1.3502 - accuracy: 0.7677\n",
      "Epoch 13/100\n",
      " - 2s - loss: 1.2713 - accuracy: 0.8062\n",
      "Epoch 14/100\n",
      " - 2s - loss: 1.2049 - accuracy: 0.8400\n",
      "Epoch 15/100\n",
      " - 2s - loss: 1.1417 - accuracy: 0.8462\n",
      "Epoch 16/100\n",
      " - 2s - loss: 1.0734 - accuracy: 0.8769\n",
      "Epoch 17/100\n",
      " - 2s - loss: 1.0166 - accuracy: 0.8954\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.9661 - accuracy: 0.9062\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.9215 - accuracy: 0.9169\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.8764 - accuracy: 0.9215\n",
      "Epoch 21/100\n",
      " - 2s - loss: 0.8283 - accuracy: 0.9338\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.7861 - accuracy: 0.9338\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.7524 - accuracy: 0.9385\n",
      "Epoch 24/100\n",
      " - 2s - loss: 0.7142 - accuracy: 0.9477\n",
      "Epoch 25/100\n",
      " - 2s - loss: 0.6791 - accuracy: 0.9385\n",
      "Epoch 26/100\n",
      " - 2s - loss: 0.6444 - accuracy: 0.9462\n",
      "Epoch 27/100\n",
      " - 2s - loss: 0.6113 - accuracy: 0.9538\n",
      "Epoch 28/100\n",
      " - 2s - loss: 0.5824 - accuracy: 0.9508\n",
      "Epoch 29/100\n",
      " - 2s - loss: 0.5535 - accuracy: 0.9569\n",
      "Epoch 30/100\n",
      " - 2s - loss: 0.5263 - accuracy: 0.9600\n",
      "Epoch 31/100\n",
      " - 2s - loss: 0.5024 - accuracy: 0.9631\n",
      "Epoch 32/100\n",
      " - 2s - loss: 0.4793 - accuracy: 0.9631\n",
      "Epoch 33/100\n",
      " - 2s - loss: 0.4540 - accuracy: 0.9646\n",
      "Epoch 34/100\n",
      " - 2s - loss: 0.4385 - accuracy: 0.9677\n",
      "Epoch 35/100\n",
      " - 2s - loss: 0.4116 - accuracy: 0.9662\n",
      "Epoch 36/100\n",
      " - 2s - loss: 0.3921 - accuracy: 0.9677\n",
      "Epoch 37/100\n",
      " - 2s - loss: 0.3724 - accuracy: 0.9692\n",
      "Epoch 38/100\n",
      " - 2s - loss: 0.3515 - accuracy: 0.9723\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.3370 - accuracy: 0.9708\n",
      "Epoch 40/100\n",
      " - 2s - loss: 0.3236 - accuracy: 0.9800\n",
      "Epoch 41/100\n",
      " - 2s - loss: 0.3092 - accuracy: 0.9723\n",
      "Epoch 42/100\n",
      " - 2s - loss: 0.2857 - accuracy: 0.9815\n",
      "Epoch 43/100\n",
      " - 2s - loss: 0.2709 - accuracy: 0.9846\n",
      "Epoch 44/100\n",
      " - 2s - loss: 0.2585 - accuracy: 0.9846\n",
      "Epoch 45/100\n",
      " - 2s - loss: 0.2417 - accuracy: 0.9862\n",
      "Epoch 46/100\n",
      " - 2s - loss: 0.2307 - accuracy: 0.9831\n",
      "Epoch 47/100\n",
      " - 2s - loss: 0.2202 - accuracy: 0.9831\n",
      "Epoch 48/100\n",
      " - 2s - loss: 0.2084 - accuracy: 0.9862\n",
      "Epoch 49/100\n",
      " - 2s - loss: 0.1978 - accuracy: 0.9892\n",
      "Epoch 50/100\n",
      " - 2s - loss: 0.1867 - accuracy: 0.9877\n",
      "Epoch 51/100\n",
      " - 2s - loss: 0.1811 - accuracy: 0.9877\n",
      "Epoch 52/100\n",
      " - 2s - loss: 0.1711 - accuracy: 0.9892\n",
      "Epoch 53/100\n",
      " - 2s - loss: 0.1627 - accuracy: 0.9892\n",
      "Epoch 54/100\n",
      " - 2s - loss: 0.1556 - accuracy: 0.9892\n",
      "Epoch 55/100\n",
      " - 2s - loss: 0.1465 - accuracy: 0.9908\n",
      "Epoch 56/100\n",
      " - 2s - loss: 0.1395 - accuracy: 0.9908\n",
      "Epoch 57/100\n",
      " - 2s - loss: 0.1336 - accuracy: 0.9923\n",
      "Epoch 58/100\n",
      " - 2s - loss: 0.1272 - accuracy: 0.9892\n",
      "Epoch 59/100\n",
      " - 2s - loss: 0.1208 - accuracy: 0.9892\n",
      "Epoch 60/100\n",
      " - 2s - loss: 0.1168 - accuracy: 0.9923\n",
      "Epoch 61/100\n",
      " - 2s - loss: 0.1102 - accuracy: 0.9908\n",
      "Epoch 62/100\n",
      " - 2s - loss: 0.1048 - accuracy: 0.9908\n",
      "Epoch 63/100\n",
      " - 2s - loss: 0.1002 - accuracy: 0.9938\n",
      "Epoch 64/100\n",
      " - 2s - loss: 0.0969 - accuracy: 0.9923\n",
      "Epoch 65/100\n",
      " - 2s - loss: 0.0923 - accuracy: 0.9954\n",
      "Epoch 66/100\n",
      " - 2s - loss: 0.0863 - accuracy: 0.9923\n",
      "Epoch 67/100\n",
      " - 2s - loss: 0.0827 - accuracy: 0.9923\n",
      "Epoch 68/100\n",
      " - 2s - loss: 0.0790 - accuracy: 0.9938\n",
      "Epoch 69/100\n",
      " - 2s - loss: 0.0743 - accuracy: 0.9954\n",
      "Epoch 70/100\n",
      " - 2s - loss: 0.0719 - accuracy: 0.9938\n",
      "Epoch 71/100\n",
      " - 2s - loss: 0.0681 - accuracy: 0.9954\n",
      "Epoch 72/100\n",
      " - 2s - loss: 0.0651 - accuracy: 0.9985\n",
      "Epoch 73/100\n",
      " - 2s - loss: 0.0634 - accuracy: 0.9985\n",
      "Epoch 74/100\n",
      " - 2s - loss: 0.0589 - accuracy: 0.9985\n",
      "Epoch 75/100\n",
      " - 2s - loss: 0.0580 - accuracy: 0.9985\n",
      "Epoch 76/100\n",
      " - 2s - loss: 0.0544 - accuracy: 0.9985\n",
      "Epoch 77/100\n",
      " - 2s - loss: 0.0519 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      " - 2s - loss: 0.0496 - accuracy: 0.9985\n",
      "Epoch 79/100\n",
      " - 2s - loss: 0.0488 - accuracy: 0.9985\n",
      "Epoch 80/100\n",
      " - 2s - loss: 0.0460 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      " - 2s - loss: 0.0443 - accuracy: 0.9985\n",
      "Epoch 82/100\n",
      " - 2s - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      " - 2s - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      " - 2s - loss: 0.0384 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      " - 2s - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      " - 2s - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      " - 2s - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      " - 2s - loss: 0.0325 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      " - 2s - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      " - 2s - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      " - 2s - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      " - 2s - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      " - 2s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      " - 2s - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      " - 2s - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      " - 2s - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      " - 2s - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      " - 2s - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      " - 2s - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      " - 2s - loss: 0.0205 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd7f04be6a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelling\n",
    "vocab_size= len(alphabet) + 1 \n",
    "embedding_vector_length= 4\n",
    "\n",
    "\n",
    "model= Sequential()\n",
    "model.add( Embedding(vocab_size, embedding_vector_length, input_length= seq_length) )\n",
    "model.add( LSTM(32) ) \n",
    "model.add( Dense(y.shape[1], activation= 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "model.fit(X, y, epochs= 100, batch_size= 10, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650/650 [==============================] - 0s 301us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01830563293913236, 1.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A'] -> B\n",
      "['B'] -> C\n",
      "['C'] -> D\n",
      "['D'] -> E\n",
      "['E'] -> F\n",
      "['F'] -> G\n",
      "['G'] -> H\n",
      "['H'] -> I\n",
      "['I'] -> J\n",
      "['J'] -> K\n",
      "['K'] -> L\n",
      "['L'] -> M\n",
      "['M'] -> N\n",
      "['N'] -> O\n",
      "['O'] -> P\n",
      "['P'] -> Q\n",
      "['Q'] -> R\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['T'] -> U\n",
      "['U'] -> V\n",
      "['V'] -> W\n",
      "['W'] -> X\n",
      "['X'] -> Y\n",
      "['Y'] -> Z\n",
      "['Z'] -> A\n",
      "['A', 'B'] -> C\n",
      "['B', 'C'] -> D\n",
      "['C', 'D'] -> E\n",
      "['D', 'E'] -> F\n",
      "['E', 'F'] -> G\n",
      "['F', 'G'] -> H\n",
      "['G', 'H'] -> I\n",
      "['H', 'I'] -> J\n",
      "['I', 'J'] -> K\n",
      "['J', 'K'] -> L\n",
      "['K', 'L'] -> M\n",
      "['L', 'M'] -> N\n",
      "['M', 'N'] -> O\n",
      "['N', 'O'] -> P\n",
      "['O', 'P'] -> Q\n",
      "['P', 'Q'] -> R\n",
      "['Q', 'R'] -> S\n",
      "['R', 'S'] -> T\n",
      "['S', 'T'] -> U\n",
      "['T', 'U'] -> V\n",
      "['U', 'V'] -> W\n",
      "['V', 'W'] -> X\n",
      "['W', 'X'] -> Y\n",
      "['X', 'Y'] -> Z\n",
      "['Y', 'Z'] -> A\n",
      "['Z', 'A'] -> B\n",
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> W\n",
      "['U', 'V', 'W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['W', 'X', 'Y'] -> Z\n",
      "['X', 'Y', 'Z'] -> A\n",
      "['Y', 'Z', 'A'] -> B\n",
      "['Z', 'A', 'B'] -> C\n",
      "['A', 'B', 'C', 'D'] -> E\n",
      "['B', 'C', 'D', 'E'] -> F\n",
      "['C', 'D', 'E', 'F'] -> G\n",
      "['D', 'E', 'F', 'G'] -> H\n",
      "['E', 'F', 'G', 'H'] -> I\n",
      "['F', 'G', 'H', 'I'] -> J\n",
      "['G', 'H', 'I', 'J'] -> K\n",
      "['H', 'I', 'J', 'K'] -> L\n",
      "['I', 'J', 'K', 'L'] -> M\n",
      "['J', 'K', 'L', 'M'] -> N\n",
      "['K', 'L', 'M', 'N'] -> O\n",
      "['L', 'M', 'N', 'O'] -> P\n",
      "['M', 'N', 'O', 'P'] -> Q\n",
      "['N', 'O', 'P', 'Q'] -> R\n",
      "['O', 'P', 'Q', 'R'] -> S\n",
      "['P', 'Q', 'R', 'S'] -> T\n",
      "['Q', 'R', 'S', 'T'] -> U\n",
      "['R', 'S', 'T', 'U'] -> V\n",
      "['S', 'T', 'U', 'V'] -> W\n",
      "['T', 'U', 'V', 'W'] -> X\n",
      "['U', 'V', 'W', 'X'] -> Y\n",
      "['V', 'W', 'X', 'Y'] -> Z\n",
      "['W', 'X', 'Y', 'Z'] -> A\n",
      "['X', 'Y', 'Z', 'A'] -> B\n",
      "['Y', 'Z', 'A', 'B'] -> C\n",
      "['Z', 'A', 'B', 'C'] -> D\n",
      "['A', 'B', 'C', 'D', 'E'] -> F\n",
      "['B', 'C', 'D', 'E', 'F'] -> G\n",
      "['C', 'D', 'E', 'F', 'G'] -> H\n",
      "['D', 'E', 'F', 'G', 'H'] -> I\n",
      "['E', 'F', 'G', 'H', 'I'] -> J\n",
      "['F', 'G', 'H', 'I', 'J'] -> K\n",
      "['G', 'H', 'I', 'J', 'K'] -> L\n",
      "['H', 'I', 'J', 'K', 'L'] -> M\n",
      "['I', 'J', 'K', 'L', 'M'] -> N\n",
      "['J', 'K', 'L', 'M', 'N'] -> O\n",
      "['K', 'L', 'M', 'N', 'O'] -> P\n",
      "['L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['R', 'S', 'T', 'U', 'V'] -> W\n",
      "['S', 'T', 'U', 'V', 'W'] -> X\n",
      "['T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] -> Z\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] -> A\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A'] -> B\n",
      "['D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B'] -> C\n",
      "['E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C'] -> D\n",
      "['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D'] -> E\n",
      "['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E'] -> F\n",
      "['H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F'] -> G\n",
      "['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'] -> H\n",
      "['J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] -> I\n",
      "['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] -> J\n",
      "['L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] -> K\n",
      "['M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'] -> L\n",
      "['N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'] -> M\n",
      "['O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M'] -> N\n",
      "['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N'] -> O\n",
      "['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'] -> P\n",
      "['R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'] -> Q\n",
      "['S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q'] -> R\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R'] -> S\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'] -> T\n",
      "['V', 'W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] -> U\n",
      "['W', 'X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V'] -> W\n",
      "['Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W'] -> X\n",
      "['Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X'] -> Y\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "for x in X:\n",
    "    #pdb.set_trace()\n",
    "    x= np.reshape( x, (1, seq_length))\n",
    "    predicted= model.predict_classes(x)\n",
    "    seq= [int_to_char[el] for el in x[0]]\n",
    "    seq= list( ''.join(seq) ) # for removing empty spaces\n",
    "    output= int_to_char[predicted[0]]\n",
    "    print(seq, '->', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'G', 'G', 'G', 'H', 'H', 'H', 'H', 'H', 'L', 'L', 'L', 'L', 'M', 'N', 'O', 'P', 'R'] -> R\n"
     ]
    }
   ],
   "source": [
    "# let's give it some unusual sequences \n",
    "\n",
    "sample= list('FGGGHHHHHLLLLMNOPR')\n",
    "x= [char_to_int[el] for el in sample]\n",
    "x= np.reshape(x, (1, len(x)))\n",
    "# pad it\n",
    "x= sequence.pad_sequences(x, maxlen= 25, value= 26)\n",
    "output= model.predict_classes(x)\n",
    "output= int_to_char[output[0]]\n",
    "print( sample, '->', output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we can see that we built a model which can totally learn te whole alphabet with 100 % accuracy.\n",
    "\n",
    "### Even when we feed the model with starnge sequences such as the above, its output is quite reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
